{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "195_Generating_LSTM_Sentences_YoungPrince_Korean_next_word.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZyxyanlZ5aJp"
      },
      "source": [
        "# 195. Keras API 와 LSTM 을 이용한 한글 어린왕자 문장 생성기\n",
        "\n",
        "- next word 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BOwsuGQQY9OL",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow.keras.regularizers as regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZimSMFiHv_pL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('alice.txt', \"https://github.com/ironmanciti/DSME/raw/master/datasets/%EC%96%B4%EB%A6%B0%EC%99%95%EC%9E%90-dmsah10.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n19R9Jpy5aJu",
        "outputId": "02b18737-1c1a-42fd-89b9-c69773b3633b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "r = open(path_to_file, 'r', encoding='cp949')\n",
        "\n",
        "texts = r.readlines()\n",
        "lines = []\n",
        "\n",
        "for line in texts:\n",
        "    line = line.strip().lower()\n",
        "    if len(line) == 0:\n",
        "        continue\n",
        "    lines.append(line)\n",
        "\n",
        "text = \" \".join(lines)\n",
        "text[:1000]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'여섯 살 적에 나는 \"체험한 이야기\"라는 제목의, 원시림에 관한 책에서 기막힌 그림 하나를 본 적이 있다. 맹수를 집어삼키고 있는 보아 구렁이 그림이었다. 위의 그림은 그것을 옮겨 그린 것이다. 그 책에는 이렇게 씌어 있었다. \"보아 구렁이는 먹이를 씹지도 않고 통째로 집어삼킨다.그리고는 꼼짝도 하지 못하고 여섯 달 동안 잠을 자면서 그것을 소화시킨다.\" 나는 그래서 밀림 속에서의 모험에 대해 한참 생각해 보고 난 끝에 색연필을 가지고 내 나름대로 내 생애 첫번째 그림을 그려보았다. 나의 그림 제 1호였다. 그것은 이런 그림이었다. 나는 그 걸작품을 어른들에게 보여 주면서내 그림이 무섭지 않느냐고 물었다. 그들은 \"모자가 뭐가 무섭다는 거니?\" 하고 대답했다. 내 그림은 모자를 그린 게 아니었다. 그것은 코끼리를 소화시키고 있는 보아 구렁이었다. 그래서 나는 어른들이 알아볼 수 있도록 보아 구렁이의 속을 그렸다. 어른들은 언제나 설명을 해주어야만 한다. 나의 그림 제 2호는 이러했다. 어른들은 속이 보이거나 보이지 않거나 하는 보아 구렁이의 그림들은 집어치우고 차라리 지리, 역사, 계산, 그리고 문법 쪽에 관심을 가져보는 게 좋을 것이라고 충고해 주었다. 그래서 나는 여섯 살 적에 화가라는 멋진 직업을 포기해 버렸다.내 그림제 1호와 제 2호가 성공을 거두지 못한 데 낙심해 버렸던 것이다. 어른들은언제나 스스로는 아무것도 이해하지 못한다.자꾸자꾸 설명을 해주어야 하니 맥빠지는 노릇이 아닐 수 없다. 그래서 다른 직업을 선택하지 않을 수 없게 된 나는 비행기 조종하는 법을배웠다.세계의 여기저기 거의 안 가본 데 없이 나는 날아다녔다.그러니지리는 정말로 많은 도움을 준 셈이었다.한번 슬쩍 보고도 중국과 애리조나를 나는 구별할 수 있었던 것이다.그것은 밤에 길을 잃었을 때 아주 유용한 일이다. 나는 그리하여 일생 동안 수없이 많은 점잖은 사람들과수많은 접촉을 가져왔다.어른들 틈에서 많이 살아온 것이다.나는 가까이서 그들을 볼 수있었다. 그렇다고 해서 그들에 대한 내'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7SZhigNh5aJx",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "corpus = re.split('[,.]', text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zk-Kn9785aJ0",
        "outputId": "00afac04-f199-48d6-e8f5-266ffe92bcf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "corpus[:10]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['여섯 살 적에 나는 \"체험한 이야기\"라는 제목의',\n",
              " ' 원시림에 관한 책에서 기막힌 그림 하나를 본 적이 있다',\n",
              " ' 맹수를 집어삼키고 있는 보아 구렁이 그림이었다',\n",
              " ' 위의 그림은 그것을 옮겨 그린 것이다',\n",
              " ' 그 책에는 이렇게 씌어 있었다',\n",
              " ' \"보아 구렁이는 먹이를 씹지도 않고 통째로 집어삼킨다',\n",
              " '그리고는 꼼짝도 하지 못하고 여섯 달 동안 잠을 자면서 그것을 소화시킨다',\n",
              " '\" 나는 그래서 밀림 속에서의 모험에 대해 한참 생각해 보고 난 끝에 색연필을 가지고 내 나름대로 내 생애 첫번째 그림을 그려보았다',\n",
              " ' 나의 그림 제 1호였다',\n",
              " ' 그것은 이런 그림이었다']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B0aXEN_15aJ2",
        "outputId": "fbb83c1e-2aeb-49ce-f6dc-ae4ad194eeda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "print(total_words)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rn-AIOff5aJ4",
        "colab": {}
      },
      "source": [
        "# create input sequences using list of tokens\n",
        "input_sequences = []\n",
        "\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tsplC63C5aJ6",
        "outputId": "4297345b-e66c-4b9c-d787-990936b41aed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "print(len(input_sequences))\n",
        "input_sequences[:10]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9365\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[142, 245],\n",
              " [142, 245, 371],\n",
              " [142, 245, 371, 3],\n",
              " [142, 245, 371, 3, 1322],\n",
              " [142, 245, 371, 3, 1322, 739],\n",
              " [142, 245, 371, 3, 1322, 739, 372],\n",
              " [142, 245, 371, 3, 1322, 739, 372, 1323],\n",
              " [1324, 1325],\n",
              " [1324, 1325, 1326],\n",
              " [1324, 1325, 1326, 1327]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-kSzQM4p5aJ8",
        "outputId": "d401f744-e587-4af5-bbaf-74cb7cc2597d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "input_sequences"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,    0,  142,  245],\n",
              "       [   0,    0,    0, ...,  142,  245,  371],\n",
              "       [   0,    0,    0, ...,  245,  371,    3],\n",
              "       ...,\n",
              "       [   0,    0,    0, ..., 4591, 1139, 4592],\n",
              "       [   0,    0,    0, ..., 1139, 4592, 4593],\n",
              "       [   0,    0,    0, ..., 4592, 4593,  525]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yU3N25jx5aJ-",
        "colab": {}
      },
      "source": [
        "text_dataset = tf.data.Dataset.from_tensor_slices(input_sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YWPQPgs05aKB",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[-1]\n",
        "    return input_text, target_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PlE-4oP55aKF",
        "outputId": "4a4b7385-5713-418e-e403-6518119426fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "source": [
        "dataset = text_dataset.map(split_input_target).batch(256, drop_remainder=True)\n",
        "\n",
        "for input, target in dataset.take(1):\n",
        "  print(input)\n",
        "  print()\n",
        "  print(target)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[   0    0    0 ...    0    0  142]\n",
            " [   0    0    0 ...    0  142  245]\n",
            " [   0    0    0 ...  142  245  371]\n",
            " ...\n",
            " [   0    0    0 ...  128  189 1418]\n",
            " [   0    0    0 ...  189 1418 1419]\n",
            " [   0    0    0 ...    0    0   23]], shape=(256, 51), dtype=int32)\n",
            "\n",
            "tf.Tensor(\n",
            "[ 245  371    3 1322  739  372 1323 1325 1326 1327  207  740   87  126\n",
            "  143 1329   10  113  373  741  298  144  742  246   13  743   61 1330\n",
            "   20  744 1331 1332  186 1333 1334  745   43  493  142 1335  299  247\n",
            " 1336  144 1337   17 1338 1339 1340  114  374  300  167   14 1341 1342\n",
            "   91   11 1343   11 1344  746  145 1345  207  248 1346  168  741    2\n",
            " 1347  747  748 1348 1349  749 1350   33  750  751 1351  494   21   28\n",
            "  298  375  246   26  376  752 1352   10  113 1353    3  495  496    9\n",
            " 1354  113  497 1355  249  127  498 1356  147  207  248 1357 1358  301\n",
            "  499   81  753   27  113  497 1359 1360 1361  754 1362 1363 1364  757\n",
            "   26  758  759 1365  377    3  142  245  371 1366  500  501  502  250\n",
            " 1367 1368  248 1369 1370 1371  503  251 1372 1373   13 1375  106  504\n",
            "  760  498  761  762 1377 1378 1379    9  252   39  501 1380  169    9\n",
            "  302  115    3 1381 1382 1383 1385  763  253 1386  251  505    3 1387\n",
            "  208   88 1389  148 1390  764 1391 1392 1393    3  765    9  187   13\n",
            "  506  149 1394  116   34 1395  766  254 1396  299 1397   88 1398 1399\n",
            " 1400 1401 1402  768 1403   13 1404  209  210 1405  255 1407  303   11\n",
            "  188 1408   63 1409  304  507  769 1410  770 1411 1412  379 1413   76\n",
            "  207  248 1414   91    2  507 1415 1416   57   29 1417  256  128  189\n",
            " 1418 1419   13 1420], shape=(256,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PRnDnCW-Z7qv",
        "outputId": "131911ea-a457-4119-ec52-799633e29586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100))\n",
        "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(16))\n",
        "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 100)         459400    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 64)          34048     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 64)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 16)                5184      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2297)              39049     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4594)              10557012  \n",
            "=================================================================\n",
            "Total params: 11,094,693\n",
            "Trainable params: 11,094,693\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AIg2f1HBxqof",
        "outputId": "cea90e5a-01ee-48a0-88a0-f5f52273124a",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " %%time\n",
        " history = model.fit(dataset, epochs=200, verbose=1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "36/36 [==============================] - 1s 27ms/step - loss: 8.4011 - accuracy: 0.0094\n",
            "Epoch 2/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 7.8159 - accuracy: 0.0110\n",
            "Epoch 3/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 7.7195 - accuracy: 0.0119\n",
            "Epoch 4/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 7.5565 - accuracy: 0.0097\n",
            "Epoch 5/200\n",
            "36/36 [==============================] - 1s 25ms/step - loss: 7.4680 - accuracy: 0.0098\n",
            "Epoch 6/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 7.4109 - accuracy: 0.0097\n",
            "Epoch 7/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 7.3669 - accuracy: 0.0097\n",
            "Epoch 8/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 7.3188 - accuracy: 0.0076\n",
            "Epoch 9/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 7.2670 - accuracy: 0.0084\n",
            "Epoch 10/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 7.2017 - accuracy: 0.0095\n",
            "Epoch 11/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 7.1518 - accuracy: 0.0109\n",
            "Epoch 12/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 7.1108 - accuracy: 0.0115\n",
            "Epoch 13/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 7.0735 - accuracy: 0.0125\n",
            "Epoch 14/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 7.0302 - accuracy: 0.0129\n",
            "Epoch 15/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 6.9868 - accuracy: 0.0142\n",
            "Epoch 16/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 6.9522 - accuracy: 0.0170\n",
            "Epoch 17/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 6.9057 - accuracy: 0.0175\n",
            "Epoch 18/200\n",
            "36/36 [==============================] - 1s 27ms/step - loss: 6.8783 - accuracy: 0.0183\n",
            "Epoch 19/200\n",
            "36/36 [==============================] - 1s 27ms/step - loss: 6.8571 - accuracy: 0.0197\n",
            "Epoch 20/200\n",
            "36/36 [==============================] - 1s 27ms/step - loss: 6.8398 - accuracy: 0.0205\n",
            "Epoch 21/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 6.8113 - accuracy: 0.0240\n",
            "Epoch 22/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 6.7531 - accuracy: 0.0221\n",
            "Epoch 23/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 6.7254 - accuracy: 0.0224\n",
            "Epoch 24/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 6.6931 - accuracy: 0.0240\n",
            "Epoch 25/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 6.6604 - accuracy: 0.0238\n",
            "Epoch 26/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 6.6277 - accuracy: 0.0229\n",
            "Epoch 27/200\n",
            "36/36 [==============================] - 1s 25ms/step - loss: 6.6057 - accuracy: 0.0226\n",
            "Epoch 28/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 6.5744 - accuracy: 0.0273\n",
            "Epoch 29/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 6.5367 - accuracy: 0.0277\n",
            "Epoch 30/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 6.5343 - accuracy: 0.0275\n",
            "Epoch 31/200\n",
            "36/36 [==============================] - 1s 27ms/step - loss: 6.5454 - accuracy: 0.0256\n",
            "Epoch 32/200\n",
            "36/36 [==============================] - 1s 27ms/step - loss: 6.5622 - accuracy: 0.0256\n",
            "Epoch 33/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 6.5335 - accuracy: 0.0276\n",
            "Epoch 34/200\n",
            "36/36 [==============================] - 1s 27ms/step - loss: 6.4578 - accuracy: 0.0361\n",
            "Epoch 35/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 6.3688 - accuracy: 0.0368\n",
            "Epoch 36/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 6.2862 - accuracy: 0.0396\n",
            "Epoch 37/200\n",
            "36/36 [==============================] - 1s 27ms/step - loss: 6.2254 - accuracy: 0.0405\n",
            "Epoch 38/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 6.1787 - accuracy: 0.0396\n",
            "Epoch 39/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 6.1357 - accuracy: 0.0398\n",
            "Epoch 40/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 6.1059 - accuracy: 0.0436\n",
            "Epoch 41/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 6.0672 - accuracy: 0.0426\n",
            "Epoch 42/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 6.0459 - accuracy: 0.0437\n",
            "Epoch 43/200\n",
            "36/36 [==============================] - 1s 25ms/step - loss: 6.0412 - accuracy: 0.0429\n",
            "Epoch 44/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 6.0179 - accuracy: 0.0452\n",
            "Epoch 45/200\n",
            "36/36 [==============================] - 1s 25ms/step - loss: 5.9919 - accuracy: 0.0458\n",
            "Epoch 46/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.9733 - accuracy: 0.0464\n",
            "Epoch 47/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.9367 - accuracy: 0.0493\n",
            "Epoch 48/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.9066 - accuracy: 0.0489\n",
            "Epoch 49/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.8962 - accuracy: 0.0482\n",
            "Epoch 50/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.8567 - accuracy: 0.0530\n",
            "Epoch 51/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.8273 - accuracy: 0.0546\n",
            "Epoch 52/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.8229 - accuracy: 0.0558\n",
            "Epoch 53/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.7698 - accuracy: 0.0570\n",
            "Epoch 54/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.7240 - accuracy: 0.0581\n",
            "Epoch 55/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.7146 - accuracy: 0.0562\n",
            "Epoch 56/200\n",
            "36/36 [==============================] - 1s 25ms/step - loss: 5.6688 - accuracy: 0.0583\n",
            "Epoch 57/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.6081 - accuracy: 0.0600\n",
            "Epoch 58/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.5482 - accuracy: 0.0622\n",
            "Epoch 59/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.4887 - accuracy: 0.0645\n",
            "Epoch 60/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.4434 - accuracy: 0.0674\n",
            "Epoch 61/200\n",
            "36/36 [==============================] - 1s 28ms/step - loss: 5.4189 - accuracy: 0.0671\n",
            "Epoch 62/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.3873 - accuracy: 0.0669\n",
            "Epoch 63/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.3533 - accuracy: 0.0688\n",
            "Epoch 64/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.3140 - accuracy: 0.0719\n",
            "Epoch 65/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.2846 - accuracy: 0.0738\n",
            "Epoch 66/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.2570 - accuracy: 0.0729\n",
            "Epoch 67/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.2157 - accuracy: 0.0763\n",
            "Epoch 68/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.1776 - accuracy: 0.0765\n",
            "Epoch 69/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.1314 - accuracy: 0.0787\n",
            "Epoch 70/200\n",
            "36/36 [==============================] - 1s 25ms/step - loss: 5.0905 - accuracy: 0.0805\n",
            "Epoch 71/200\n",
            "36/36 [==============================] - 1s 27ms/step - loss: 5.0687 - accuracy: 0.0830\n",
            "Epoch 72/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 5.0205 - accuracy: 0.0870\n",
            "Epoch 73/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.9884 - accuracy: 0.0855\n",
            "Epoch 74/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.9674 - accuracy: 0.0888\n",
            "Epoch 75/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.9345 - accuracy: 0.0918\n",
            "Epoch 76/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.8916 - accuracy: 0.0940\n",
            "Epoch 77/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.8591 - accuracy: 0.0952\n",
            "Epoch 78/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.8489 - accuracy: 0.0944\n",
            "Epoch 79/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.8278 - accuracy: 0.0978\n",
            "Epoch 80/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.7996 - accuracy: 0.0972\n",
            "Epoch 81/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.7802 - accuracy: 0.0993\n",
            "Epoch 82/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.7429 - accuracy: 0.1042\n",
            "Epoch 83/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.6851 - accuracy: 0.1070\n",
            "Epoch 84/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.6528 - accuracy: 0.1085\n",
            "Epoch 85/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.6235 - accuracy: 0.1137\n",
            "Epoch 86/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.5837 - accuracy: 0.1165\n",
            "Epoch 87/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.5492 - accuracy: 0.1183\n",
            "Epoch 88/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.5177 - accuracy: 0.1235\n",
            "Epoch 89/200\n",
            "36/36 [==============================] - 1s 27ms/step - loss: 4.5061 - accuracy: 0.1224\n",
            "Epoch 90/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.4777 - accuracy: 0.1248\n",
            "Epoch 91/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.4573 - accuracy: 0.1260\n",
            "Epoch 92/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.4245 - accuracy: 0.1275\n",
            "Epoch 93/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.3989 - accuracy: 0.1322\n",
            "Epoch 94/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.3955 - accuracy: 0.1324\n",
            "Epoch 95/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.3463 - accuracy: 0.1432\n",
            "Epoch 96/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.3286 - accuracy: 0.1426\n",
            "Epoch 97/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.2871 - accuracy: 0.1501\n",
            "Epoch 98/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.2601 - accuracy: 0.1514\n",
            "Epoch 99/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.2333 - accuracy: 0.1535\n",
            "Epoch 100/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.2055 - accuracy: 0.1590\n",
            "Epoch 101/200\n",
            "36/36 [==============================] - 1s 25ms/step - loss: 4.1748 - accuracy: 0.1629\n",
            "Epoch 102/200\n",
            "36/36 [==============================] - 1s 25ms/step - loss: 4.1238 - accuracy: 0.1685\n",
            "Epoch 103/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.0845 - accuracy: 0.1781\n",
            "Epoch 104/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.0521 - accuracy: 0.1807\n",
            "Epoch 105/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 4.0146 - accuracy: 0.1841\n",
            "Epoch 106/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.9791 - accuracy: 0.1927\n",
            "Epoch 107/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.9315 - accuracy: 0.1995\n",
            "Epoch 108/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.9025 - accuracy: 0.2035\n",
            "Epoch 109/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.8710 - accuracy: 0.2119\n",
            "Epoch 110/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.8539 - accuracy: 0.2139\n",
            "Epoch 111/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.8299 - accuracy: 0.2159\n",
            "Epoch 112/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.7821 - accuracy: 0.2278\n",
            "Epoch 113/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.7494 - accuracy: 0.2340\n",
            "Epoch 114/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.7428 - accuracy: 0.2330\n",
            "Epoch 115/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.7264 - accuracy: 0.2374\n",
            "Epoch 116/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.7170 - accuracy: 0.2418\n",
            "Epoch 117/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.7017 - accuracy: 0.2450\n",
            "Epoch 118/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.6564 - accuracy: 0.2511\n",
            "Epoch 119/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.6255 - accuracy: 0.2593\n",
            "Epoch 120/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.5930 - accuracy: 0.2623\n",
            "Epoch 121/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.5677 - accuracy: 0.2673\n",
            "Epoch 122/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.5428 - accuracy: 0.2741\n",
            "Epoch 123/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.5046 - accuracy: 0.2816\n",
            "Epoch 124/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.4988 - accuracy: 0.2821\n",
            "Epoch 125/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.4828 - accuracy: 0.2855\n",
            "Epoch 126/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.4647 - accuracy: 0.2932\n",
            "Epoch 127/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.4558 - accuracy: 0.2939\n",
            "Epoch 128/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.4330 - accuracy: 0.3021\n",
            "Epoch 129/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.3767 - accuracy: 0.3109\n",
            "Epoch 130/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.3595 - accuracy: 0.3158\n",
            "Epoch 131/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.3161 - accuracy: 0.3292\n",
            "Epoch 132/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.2800 - accuracy: 0.3334\n",
            "Epoch 133/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.2546 - accuracy: 0.3455\n",
            "Epoch 134/200\n",
            "36/36 [==============================] - 1s 27ms/step - loss: 3.2319 - accuracy: 0.3456\n",
            "Epoch 135/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.2129 - accuracy: 0.3486\n",
            "Epoch 136/200\n",
            "36/36 [==============================] - 1s 25ms/step - loss: 3.1754 - accuracy: 0.3557\n",
            "Epoch 137/200\n",
            "36/36 [==============================] - 1s 25ms/step - loss: 3.1650 - accuracy: 0.3615\n",
            "Epoch 138/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.1333 - accuracy: 0.3672\n",
            "Epoch 139/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.1172 - accuracy: 0.3750\n",
            "Epoch 140/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.0926 - accuracy: 0.3754\n",
            "Epoch 141/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.0870 - accuracy: 0.3841\n",
            "Epoch 142/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.0717 - accuracy: 0.3832\n",
            "Epoch 143/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.0494 - accuracy: 0.3905\n",
            "Epoch 144/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.0546 - accuracy: 0.3864\n",
            "Epoch 145/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 3.0349 - accuracy: 0.3860\n",
            "Epoch 146/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.9922 - accuracy: 0.4025\n",
            "Epoch 147/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.9673 - accuracy: 0.4061\n",
            "Epoch 148/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.9299 - accuracy: 0.4135\n",
            "Epoch 149/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.9020 - accuracy: 0.4258\n",
            "Epoch 150/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.8663 - accuracy: 0.4323\n",
            "Epoch 151/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.8497 - accuracy: 0.4358\n",
            "Epoch 152/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.8219 - accuracy: 0.4447\n",
            "Epoch 153/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.7937 - accuracy: 0.4571\n",
            "Epoch 154/200\n",
            "36/36 [==============================] - 1s 25ms/step - loss: 2.7710 - accuracy: 0.4597\n",
            "Epoch 155/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.7530 - accuracy: 0.4683\n",
            "Epoch 156/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.7393 - accuracy: 0.4663\n",
            "Epoch 157/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.7152 - accuracy: 0.4778\n",
            "Epoch 158/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.6849 - accuracy: 0.4834\n",
            "Epoch 159/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.6736 - accuracy: 0.4806\n",
            "Epoch 160/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.6428 - accuracy: 0.4956\n",
            "Epoch 161/200\n",
            "36/36 [==============================] - 1s 25ms/step - loss: 2.6170 - accuracy: 0.5005\n",
            "Epoch 162/200\n",
            "36/36 [==============================] - 1s 25ms/step - loss: 2.5858 - accuracy: 0.5118\n",
            "Epoch 163/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.5789 - accuracy: 0.5079\n",
            "Epoch 164/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.5614 - accuracy: 0.5163\n",
            "Epoch 165/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.5404 - accuracy: 0.5227\n",
            "Epoch 166/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.5373 - accuracy: 0.5179\n",
            "Epoch 167/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.5125 - accuracy: 0.5244\n",
            "Epoch 168/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.4959 - accuracy: 0.5265\n",
            "Epoch 169/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.4948 - accuracy: 0.5256\n",
            "Epoch 170/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.4777 - accuracy: 0.5314\n",
            "Epoch 171/200\n",
            "36/36 [==============================] - 1s 25ms/step - loss: 2.4707 - accuracy: 0.5382\n",
            "Epoch 172/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.4536 - accuracy: 0.5362\n",
            "Epoch 173/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.4402 - accuracy: 0.5443\n",
            "Epoch 174/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.3856 - accuracy: 0.5611\n",
            "Epoch 175/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.3639 - accuracy: 0.5588\n",
            "Epoch 176/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.3597 - accuracy: 0.5614\n",
            "Epoch 177/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.3137 - accuracy: 0.5766\n",
            "Epoch 178/200\n",
            "36/36 [==============================] - 1s 27ms/step - loss: 2.2964 - accuracy: 0.5793\n",
            "Epoch 179/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.2702 - accuracy: 0.5867\n",
            "Epoch 180/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.2475 - accuracy: 0.5970\n",
            "Epoch 181/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.2281 - accuracy: 0.6007\n",
            "Epoch 182/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.2135 - accuracy: 0.6003\n",
            "Epoch 183/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.2069 - accuracy: 0.6037\n",
            "Epoch 184/200\n",
            "36/36 [==============================] - 1s 27ms/step - loss: 2.1867 - accuracy: 0.6059\n",
            "Epoch 185/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.1963 - accuracy: 0.6049\n",
            "Epoch 186/200\n",
            "36/36 [==============================] - 1s 27ms/step - loss: 2.1604 - accuracy: 0.6141\n",
            "Epoch 187/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.1600 - accuracy: 0.6188\n",
            "Epoch 188/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.1395 - accuracy: 0.6163\n",
            "Epoch 189/200\n",
            "36/36 [==============================] - 1s 25ms/step - loss: 2.1250 - accuracy: 0.6222\n",
            "Epoch 190/200\n",
            "36/36 [==============================] - 1s 25ms/step - loss: 2.1174 - accuracy: 0.6227\n",
            "Epoch 191/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.0990 - accuracy: 0.6268\n",
            "Epoch 192/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.0965 - accuracy: 0.6261\n",
            "Epoch 193/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.0952 - accuracy: 0.6299\n",
            "Epoch 194/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.0775 - accuracy: 0.6286\n",
            "Epoch 195/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.0674 - accuracy: 0.6287\n",
            "Epoch 196/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 2.0362 - accuracy: 0.6424\n",
            "Epoch 197/200\n",
            "36/36 [==============================] - 1s 25ms/step - loss: 2.0045 - accuracy: 0.6443\n",
            "Epoch 198/200\n",
            "36/36 [==============================] - 1s 25ms/step - loss: 1.9986 - accuracy: 0.6497\n",
            "Epoch 199/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 1.9823 - accuracy: 0.6507\n",
            "Epoch 200/200\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 1.9641 - accuracy: 0.6562\n",
            "CPU times: user 4min, sys: 32.4 s, total: 4min 32s\n",
            "Wall time: 3min 27s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1fXTEO3GJ282",
        "outputId": "c7f3f94c-ce7c-4310-aa62-1d6a85114b56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.title('Training accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6Vc6PHgxa6Hm",
        "outputId": "bcf13d16-53c4-42e7-e76d-e7fa1af039f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "seed_text = \"그래서 나는 어른들이 알아볼 수 있도록 보아 구렁이의 속을 그렸다\"\n",
        "next_words = 100\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = model.predict_classes(token_list, verbose=0)\n",
        "    output_word = tokenizer.index_word[predicted[0]]\n",
        "    seed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "그래서 나는 어른들이 알아볼 수 있도록 보아 구렁이의 속을 그렸다 없다가어린 왕자가 차마 그럴 수 있는 간신히 따르게 세로 20마일 가로 20마일의 광장으로도 충분할 것이다 송이는 걸 알고 발견했다 않을 싶지 있는 일이었다 느낄 수 있었다 나는 그를 몸을 없는 보물을 식물일 의미가 있거든 바라보고 거야 것이었다 그랬어 끄적거려 말없이 잇지 못했다 데 있는 아닌가 그 별은 시 간이란다 있었으므로 어린 왕자가 말했다 말을 이었다 그렸다 아닌가 그 노인이 물었다 되물었다 물었다 보고 말들을 6십 6십 2만 2천 7백 3십 1이 되 는구나 일분이라니 조금도 는구나 면 변하도록 11명이나 되는가로등 나는 그들은 아닌가 그의 가슴이 흉측스럽게 일이 긴요한 위로 몸을 감추어 버렸다 이야기하는 것이다 금빛이니까 내가 이 여전히 그래 그러나\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OX_yyEP95aKZ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}