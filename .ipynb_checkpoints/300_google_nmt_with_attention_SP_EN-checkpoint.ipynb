{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# 300. Neural machine translation with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CiwtNgENbx2g"
   },
   "source": [
    "- [Google Tutorial](https://www.tensorflow.org/tutorials/text/nmt_with_attention?hl=ko) 참조\n",
    "- Spanish to English translation 을 seq2seq attention model 로 구현  \n",
    "- 아래의 matrix 는 model 이 문장을 번역할 때 input sequence 의 어느 부분에  attention 했는지 보여준다\n",
    "\n",
    "<img src=\"https://tensorflow.org/images/spanish-english.png\" alt=\"spanish-english attention plot\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1E47G9t7ovP1"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wfodePkj3jEa"
   },
   "source": [
    "# Step1. Download and prepare the dataset\n",
    "\n",
    "- data 는 [Tab-delimited Bilingual Sentence Pairs](http://www.manythings.org/anki/) 에서 download  \n",
    "\n",
    "\n",
    "    - English + TAB + The Other Language + TAB + Attribution 형식  \n",
    "        - Hi.\t안녕.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #8355888 (Eunhee)\n",
    "        - May I borrow this book? ¿Puedo tomar prestado este libro?\n",
    "\n",
    "- 편의상 google cloud 에서 data download 하고 다음의 전처리를 해준다.\n",
    "\n",
    "\n",
    "1.  Teacher Forcing 용 data 생성\n",
    "    - target_texts_inputs  : 1 만큼 offset 된 target language sentence $\\rightarrow$ `<sos>...`\n",
    "    - target_texts  : target language sentence  $\\rightarrow$ `.....<eos>`\n",
    "\n",
    "\n",
    "2. 특수문자 제거  \n",
    "\n",
    "\n",
    "3. 각 sentence 를 maximum 길이로 sequence padding  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRVATYOgJs1b"
   },
   "outputs": [],
   "source": [
    "# Google cloud 에서 dataset download\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', \n",
    "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "print(path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S5x8f0PAj5tA"
   },
   "source": [
    "* unicodedata category - Mn*\n",
    "\n",
    "<a href=\"https://graphemica.com/categories/nonspacing-mark\"> Unicode Characters»Categories»Nonspacing Mark</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rd0jw-eC3jEh"
   },
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "# NFD - Normalization Form Decomposition (EX. 자모를 분리하여 저장한 후 보여줄 때 합쳐서 보여주는 방식)\n",
    "# Mn - Nonspacing_Mark\n",
    "# 문장을 NFD 방식으로 normalize 하고 accent(MN) 을 제거\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "  w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "  # word 와 구둣점 사이에 space 추가 \n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  # \\1 - first group\n",
    "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "  w = w.rstrip().strip()\n",
    "\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ysLH39K0V5c"
   },
   "source": [
    "### preprocessing example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ezCFDGE9xMR1"
   },
   "outputs": [],
   "source": [
    "print(re.sub(r\"([?.!,¿])\", r\" \\1 \", u\"May I borrow this book?\"))\n",
    "print(re.sub(r\"([?.!,¿])\", r\" \\1 \", u\"¿Puedo tomar prestado este libro?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opI2GzOt479E"
   },
   "outputs": [],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "krN0QTHG1Yi0"
   },
   "source": [
    "### accent 삭제 및 white space 제거\n",
    "### [ENGLISH, SPANISH] 쌍으로 data 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHn4Dct23jEm"
   },
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfD3OS6syY_l"
   },
   "outputs": [],
   "source": [
    "lines = open(path_to_file, encoding='UTF-8').read().strip().split('\\n')\n",
    "print(lines[100:105])\n",
    "\n",
    "word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[100:105]]\n",
    "print(word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HYOiWcVenIRu"
   },
   "outputs": [],
   "source": [
    "en, sp = create_dataset(path_to_file, None)\n",
    "print(en[-1])\n",
    "print(sp[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1c8AAm0G1lht"
   },
   "source": [
    "### Tokenize\n",
    "- Tokenizer 의 filters='' 로 해주지 않으면 `<start>`, `<end>` token 이 filtering 되므로 주의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIOn8RCNDJXG"
   },
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    tensor = tokenizer.texts_to_sequences(sentences)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAY9k49G3jE_"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "  # creating cleaned input, output pairs\n",
    "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOi42V79Ydlr"
   },
   "source": [
    "### 수행 시간 감안하여 datase size 축소 (optional)\n",
    "\n",
    "100,000 개 넘는 sentence 를 모두 사용하면 오래 걸리므로 30,000 개의 sentence 만 가지고 train. (번역품질 악화 trade-off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnxC7q-j3jFD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_examples = 30000\n",
    "\n",
    "input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer \\\n",
    "            = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# target tensor 의 max_length 계산\n",
    "max_length_targ, max_length_inp \\\n",
    "            = max(len(t) for t in target_tensor), max(len(t) for t in input_tensor)\n",
    "max_length_targ, max_length_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QILQkOs3jFG"
   },
   "outputs": [],
   "source": [
    "# 80-20 split 으로 train, validation set 분리 \n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val \\\n",
    "                 = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "print(len(input_tensor_train), len(target_tensor_train))\n",
    "print(len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VXukARTDd7MT"
   },
   "outputs": [],
   "source": [
    "print (\"Input Language; \")     \n",
    "print([inp_lang_tokenizer.index_word[i] for i in input_tensor_train[0] if i != 0])\n",
    "print ()\n",
    "print (\"Target Language; \")\n",
    "print([targ_lang_tokenizer.index_word[i] for i in target_tensor_train[0] if i != 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "### tf.data dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)   # 24,000\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE    #375\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang_tokenizer.word_index) + 1       #9413+1\n",
    "vocab_tar_size = len(targ_lang_tokenizer.word_index) + 1      #4934+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qc6-NK1GtWQt"
   },
   "outputs": [],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jKsOUGrAj5tg"
   },
   "source": [
    "## Write the encoder and decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNfHIF71ulLu"
   },
   "source": [
    "\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" width=\"500\" alt=\"attention mechanism\">\n",
    "\n",
    "input 이 encoder model 을 통과하여 *(batch_size, max_length, hidden_size)* 의 encoder output 과 *(batch_size, hidden_size)* 의 encoder hidden state 가 나온다.\n",
    "\n",
    "다음의 공식 구현:\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" alt=\"attention equation 0\" width=\"600\">\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\" alt=\"attention equation 1\" width=\"600\">\n",
    "\n",
    "여기서는 decoder 에 [Bahdanau attention](https://arxiv.org/pdf/1409.0473.pdf) 사용. 다음과 같이 notation 을 정함.\n",
    "\n",
    "* FC = Fully connected (dense) layer\n",
    "* EO = Encoder output\n",
    "* H = hidden state\n",
    "* X = input to the decoder\n",
    "\n",
    "### Decoder 의 pseudo-code 는 다음과 같음:\n",
    "\n",
    "- BahdanauAttention\n",
    "    * `score = FC(tanh(FC(EO) + FC(H)))`\n",
    "    * `attention weights = softmax(score, axis = 1)`   \n",
    "    score 의 shape 이 *(batch_size, max_length, hidden_size)* 이므로, input 의 length 가 max_length 이다.  \n",
    "    우리가 하려는 것은 각 input 의 time step에 weight 를 할당하려는 것 이므로, axis=1 에 softmax 를 적용해야 한다.\n",
    "\n",
    "    * `context vector = sum(attention weights * EO, axis = 1)`. 위와 같은 이유로 axis 1 을 선택.  \n",
    "\n",
    "- Decoder\n",
    "    * `embedding output` = decoder input (target language) x 는 embedding layer 를 통과하여 embedding output 으로 출력\n",
    "    * `merged vector = concat(embedding output, context vector)`\n",
    "    * merged vector 를 GRU 의 input 으로 전달\n",
    "    * GRU 의 output 은 dense layer 를 통해 logit 구성\n",
    "    * GRU 의 hidden 은 next step attention 의 query 로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NhCzjYsxj5tg"
   },
   "source": [
    "# Step 2. Encoder model 작성\n",
    "\n",
    "- keras subclass API 사용  \n",
    "\n",
    "- attention 작성에는 hidden state 만 필요하므로 LSTM 대신 GRU 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jSkO_GgvFAe1"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                        return_sequences=True,\n",
    "                                        return_state=True,\n",
    "                                        recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden_state = self.gru(x, initial_state = hidden)\n",
    "        return output, hidden_state\n",
    "\n",
    "    def initialize_hidden_state(self):   # decoder state 0\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('모든 timestep 의 Encoder output shape: \\n(batch size, max input sequence length, units) {}'\n",
    "                       .format(sample_output.shape))\n",
    "print ('마지막 step 의 hidden state shape: \\n(batch size, units) {}'\n",
    "                       .format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60gSVh05Jl6l"
   },
   "outputs": [],
   "source": [
    "encoder.summary()\n",
    "plot_model(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "899Xic6pj5tl"
   },
   "source": [
    "### class BahdanauAttention parameters\n",
    "\n",
    "#### def call(self, query, values):  \n",
    "- query : (초기값) encoder 의 last hidden state,   \n",
    "(훈련중) decoder 의 step 별 hidden state (batch_size, hidden_unit) -> (64, 1024)  \n",
    "- values : encoder 모든 timestep 의 output (batch_size, input_lang_max_length, hidden_unit) --> (64, 16, 1024)  \n",
    "- hidden_with_time_axis 의 shape == (batch_size, 1, hidden size) -> (64, 1, 1024)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umohpBN2OM94"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  # query - (초기값) encoder 의 last hidden state, (훈련중) decoder 의 step 별 hidden state\n",
    "  # value - encoder 의 모든 timestep 의 output (64, 16, 1024)\n",
    "  def call(self, query, values):\n",
    "    \n",
    "    # score shape == (batch_size, max_length, 1) -> (64, 16, 1)  \n",
    "    # values 와 query 값을 mix 하기 위해 last axis 에 1 을 추가 \n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "    \n",
    "    score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, input_lang_max_length, 1) -> (64, 16, 1)\n",
    "    # timestep 에 대해 softmax 적용 (default 는 last exis)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)   \n",
    "\n",
    "    # context_vector(어텐션 값) 의 shape after sum == (batch_size, hidden_size)\n",
    "    # query step 에 맞는 새로운 context vector \n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)   \n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k534zTHiDjQU"
   },
   "outputs": [],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "context_vector, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Context vector shape: (batch size, hidden_units) {}\"\n",
    "                  .format(context_vector.shape))\n",
    "print(\"Attention weights shape: (batch_size, input_lang_max_sequence_length, 1) {}\"\n",
    "                  .format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DGZkk4B5j5tq"
   },
   "source": [
    "### class Decoder parameters\n",
    "\n",
    "#### def call(self, x, hidden, enc_output):\n",
    "\n",
    "- x : timestep t 의 이전 timestep output (teacher forcing 은 이전 timestep true value)  \n",
    "(batch_size, 1) -> (64, 1)  \n",
    "- hidden : encoder 의 hidden unit (64, 1024)  \n",
    "- enc_output : encoder 의  모든 timestep 의 output  \n",
    "(batch_size, input_lang_max_length, hidden_size) -> (64, 16, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)  # vocab_size - target_lang vocab_size (4935)\n",
    "\n",
    "    # Attention unit 생성\n",
    "    self.attention = BahdanauAttention(self.dec_units)        \n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    \n",
    "    # self.attention(query, value)\n",
    "    # context_vector (batch_size, hidden_size) -> (64, 1024)\n",
    "    # attention_weights (64, 14, 1)\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # x.shape - (64, 1)\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)   # (64, 1, 256)\n",
    "\n",
    "    # attention 값 (context vector) 과 timestep t 의 output 을 연결\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1) # (64, 1, 1280)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)  # (64, 1, 1024), (64, 1024)\n",
    "\n",
    "    # (batch_size, hidden_size) \n",
    "    output = tf.reshape(output, (-1, output.shape[2])) # (64, 1024)\n",
    "\n",
    "    x = self.fc(output)  # (batch_size, vocab) -> (64, 4929)\n",
    "\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ndTJS_s3j5tr"
   },
   "source": [
    "- vocab_tar_size : 4929 \n",
    "- embedding_dim : 256\n",
    "- BATCH_SIZE : 64  \n",
    "- sample_output : encoder 의 모든 timestep 의 output (64, 16, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5UY8wko3jFp"
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, target_lang_vocab size) {}'\n",
    "                       .format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pyoiEpa1j5tu"
   },
   "outputs": [],
   "source": [
    "decoder.summary()\n",
    "plot_model(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function\n",
    "\n",
    "- tf.keras.losses.SparseCategoricalCrossentropy 의 reduction 이 NONE 이면, return shape 은 [batch_size, d0, .. dN-1] 이 된다. \n",
    "- 그렇지 않으면 scalar return. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1MPPwtu4j5tv"
   },
   "source": [
    "#### def loss_function(real, pred):     \n",
    "- real : timestep t 의 target 단어(targ[:, t])\n",
    "    * shape (batch_size, 1) -> (64, 1)\n",
    "- pred : decoder 의 예측값 (predictions) \n",
    "    * shape (batch_size, target_lang_vocab_size) -> (64, 4929)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "def loss_function(real, pred):     \n",
    "\n",
    "  # batch 64 개 record 중 timestep t 에 0 padding 이 아닌 \n",
    "  # 실제 단어가 존재하는 record 만 True 로 만듦\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))    \n",
    "\n",
    "  # [word_index] 에 대한 확률분포 array - (64, ), dtype=float32 \n",
    "  loss_ = loss_object(real, pred)    \n",
    "\n",
    "  # mask dtype 을 float32 로 type cast\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)   \n",
    "\n",
    "  loss_ *= mask             # 실제 단어가 존재하는 위치 외에는 모두 0 으로 만든다\n",
    "\n",
    "  return tf.reduce_mean(loss_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PSO-SeoQj5tx"
   },
   "outputs": [],
   "source": [
    "# 참고 : tf.math.equal, logical_not\n",
    "print(tf.math.equal([12, 64, 0, 0, 85, 0, 0], 0))\n",
    "print(tf.math.logical_not(tf.math.equal([12, 64, 0, 0, 85, 0, 0], 0)))\n",
    "\n",
    "# tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) 의 return 값\n",
    "loss_ = loss_object(tf.convert_to_tensor([0, 1, 2]), \n",
    "                    tf.convert_to_tensor([[.9, .05, .05], \n",
    "                                          [.5, .89, .6], \n",
    "                                          [.05, .01, .94]]))\n",
    "print(loss_.numpy())\n",
    "\n",
    "a = loss_object(tf.convert_to_tensor([0]), tf.convert_to_tensor([.9, .05, .05])).numpy()\n",
    "b = loss_object(tf.convert_to_tensor([1]), tf.convert_to_tensor([.5, .89, .6])).numpy()\n",
    "c = loss_object(tf.convert_to_tensor([2]), tf.convert_to_tensor([.05, .01, .94])).numpy()\n",
    "\n",
    "print(a, b, c)\n",
    "print((a+b+c)/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpObfY22IddU"
   },
   "source": [
    "## Training\n",
    "\n",
    "1. *input* 을 *encoder* 에 통과시켜 *encoder output* 과 *encoder hidden state* 를 출력\n",
    "2. *encoder output*, *encoder hidden state*, *decoder input( start token*) 을 decoder 로 공급\n",
    "3. decoder 는 *prediction* 과 *decoder hidden state* 를 반환\n",
    "4. *decoder hidden state* 를 다시 model 로 되돌려지고,  *prediction* 은 loss 계산에 사용\n",
    "5. decoder 에 공급할 next input 을 *teacher forcing* 으로 결정\n",
    "6. *Teacher forcing* 은 *target word* 를 *next input*  으로  decoder 에 공급하는 기술이다.\n",
    "7. final step 은 gradient 를 계산하고 optimizer 에 적용하여  backpropagate 하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "evQU56Rbj5t1"
   },
   "source": [
    "### def train_step\n",
    "\n",
    "- inp (64, 16) - [  1  65  53 ...   0   0   0]\n",
    "- targ (64, 11) - [   1   82   13  231  449    7    2    0    0    0    0]\n",
    "\n",
    "- enc_output 의 shape (batch_size, target_lang_max_len, hidden_units) -> (64, 16, 1024)\n",
    "- enc_hidden 의 shape (batch_size, hidden_units) -> (64, 1024) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "    \n",
    "    # decoder hidden state 의 초기값은  encoder  last hidden state\n",
    "    dec_hidden = enc_hidden   \n",
    "\n",
    "    # dec_input shape (batch_size, 1) -> (64, 1) 의 <start> token 으로 시작\n",
    "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):    # targ.shape = (64, 11)\n",
    "      # passing enc_output to the decoder\n",
    "      # decoder_hidden 출력을 next decoder 의 input 으로 update\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)  # (64,) (64, 4929)\n",
    "\n",
    "      # teacher forcing - predictions 가 아닌 true value 를 \n",
    "      # next step 의 dec_input 으로 제공\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  # targ.shape -> (64, 11)\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddefjBMa3jF0"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} 평균 Loss {:.4f}'.format(epoch+1, total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format((time.time() - start)/(epoch+1)))\n",
    "\n",
    "print('Total Time {} sec'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mU3Ce8M6I3rz"
   },
   "source": [
    "## Translate\n",
    "\n",
    "* evaluate 함수는 *teacher forcing* 을 하지 않는 것만 다르고  training loop 와 유사. \n",
    "* 각 time step 에서의 decoder input 은  이전 step 의  prediction, hidden state, encoder output 이다.\n",
    "* model 이 *end token* 을 predict 하면 prediction 종료.\n",
    "* *attention weights* 를 각 time step 별로 저장.\n",
    "\n",
    "Note: encoder output 은 input sentence 에 대해 단 한번 계산된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden= [tf.zeros((1, units))]   # units : 1024\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):      # 11\n",
    "    predictions, dec_hidden, attention_weights  \\\n",
    "            = decoder(dec_input, dec_hidden, enc_out)\n",
    "\n",
    "    # 시각화를 위해 attention weights 저장 \n",
    "    attention_weights = tf.reshape(attention_weights, (-1,))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "    # greedy 하게 next word 선택\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    predicted_word = targ_lang_tokenizer.index_word[predicted_id]\n",
    "    result +=  predicted_word + ' '\n",
    "\n",
    "    if predicted_word == '<end>':\n",
    "      return result, sentence, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model - no teacher-forcing\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgJ5udiMj5t7"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "  fig = plt.figure(figsize=(8,6))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  sns.heatmap(attention, annot=True, fmt=\".2f\")\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sl9zUHzg3jGI"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))\n",
    "\n",
    "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n250XbnjOaqP"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJpT9D5_OgP6"
   },
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WrAM0FDomq3E"
   },
   "outputs": [],
   "source": [
    "translate(u'hace mucho frio aqui.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zSx2iM36EZQZ"
   },
   "outputs": [],
   "source": [
    "translate(u'esta es mi vida.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3LLCx3ZE0Ls"
   },
   "outputs": [],
   "source": [
    "translate(u'¿todavia estan en casa?')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "090_google_nmt_with_attention_SP_EN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
