{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EKOTlwcmxmej"
   },
   "source": [
    "# 145. BERT Fine-Tuning Tutorial with HuggingFace PyTorch Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJR6t_gCQe_x"
   },
   "source": [
    "- [Chris McCormick and Nick Ryan](https://mccormickml.com/2019/07/22/BERT-fine-tuning/) tutorial 을 기초로 작성함\n",
    "\n",
    "\n",
    "- Huggingface PyTorch 라이브러리와 함께 BERT를 사용하여 문장 분류에서 최첨단 성능에 근접하도록 모델을 빠르고 효율적으로 미세 조정하는 방법을 보여줍니다. 더 광범위하게, NLP에서 전이 학습을 실용적으로 적용하여 다양한 NLP 작업에서 최소한의 노력으로 고성능 모델을 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jrC9__lXxTJz"
   },
   "source": [
    "# Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ADkUGTqixRWo"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L9vxxTBsuL24"
   },
   "source": [
    "\n",
    "## History\n",
    "\n",
    "2018 년은 NLP에서 획기적인 해였습니다. 전이 학습, 특히 Allen AI의 ELMO, OpenAI의 Open-GPT 및 Google의 BERT와 같은 모델을 통해 연구원들은 최소한의 작업 별 미세 조정으로 여러 벤치 마크를 깰 수 있었고, 나머지 NLP 커뮤니티에 데이터를 적게 사용하여 쉽게 훈련 할 수있는 사전 훈련 된 모델을 제공 할 수있었습니다. 불행히도 NLP를 시작한 많은 사람들과 숙련 된 실무자들에게도이 강력한 모델의 이론과 실제 적용은 아직 잘 이해되지 않고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qCgvR9INuP5q"
   },
   "source": [
    "\n",
    "## What is BERT?\n",
    "\n",
    "2018 년 후반에 출시 된 BERT (Transformer 의 양방향 인코더 표현)는 이 자습서에서 NLP 에서 전이학습 모델을 사용하기위한 더 나은 이해와 실제 지침을 제공하기 위해 사용할 모델입니다. BERT는 NLP 실무자가 다운로드하여 무료로 사용할 수있는 모델을 만드는 데 사용된 언어 표현을 사전 훈련(pre-training)된 model 입니다. 이 모델을 사용하여 텍스트 데이터에서 고품질 언어 기능을 추출하거나 downstream task (분류, 엔터티 인식, 질문 응답 등)를 자신의 데이터로 미세 조정하여 모델을 생성 할 수 있습니다.\n",
    "\n",
    "이 자습서에서는 BERT를 수정하고 미세 조정하여 최신 결과를 신속하게 제공하는 강력한 NLP 모델을 만드는 방법을 설명합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DaVGdtOkuXUZ"
   },
   "source": [
    "\n",
    "## Fine-Tuning 의 장점\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5llwu8GBuqMb"
   },
   "source": [
    "구체적으로, 우리는 사전 훈련 된 BERT 모델을 취하고, 끝단에 훈련되지 않은 뉴런층을 추가하고, 분류 작업을 위해 새로운 모델을 훈련시킬 것입니다. 특정 NLP 작업에 적합한 특정 딥 러닝 모델 (CNN, BiLSTM 등)을 훈련 시키기보다 유리한 이유는 다음과 같습니다.\n",
    "\n",
    "1. **Quicker Development**\n",
    "\n",
    "    * 먼저, 사전 훈련 된 BERT 모델 가중치는 이미 우리 언어에 대한 많은 정보를 인코딩 합니다. 결과적으로, 미세 조정 된 모델을 학습하는 데 훨씬 적은 시간이 소요됩니다. 특정 NLP 작업에 대한 BERT 미세 조정을 위해 2-4 epoch 의 교육을 권장합니다 (원래 BERT 모델 또는 LSTM을 처음부터 교육하는 데 필요한 수백 시간의 GPU 작업 소요)\n",
    "    \n",
    "\n",
    "2. **Less Data**\n",
    "\n",
    "    * 또한 사전 훈련 된 가중치 덕분에 이 방법을 사용하면 처음부터 빌드된 모델에 필요한 것보다 훨씬 작은 데이터로 모델을 미세 조정할 수 있습니다. NLP 모델을 처음부터 작성하는 주요 단점은 네트워크를 합리적인 정확도로 훈련시키기 위해 엄청나게 큰 데이터 세트가 필요하다는 것입니다. 이는 데이터 세트 작성에 많은 시간과 에너지를 투입해야한다는 것을 의미합니다. BERT를 미세 조정함으로써 훨씬 적은 양의 데이터로 모델을 효과적으로 훈련시킬 수 있습니다.\n",
    "    \n",
    "\n",
    "3. **Better Results**\n",
    "\n",
    "    * 마지막으로,이 간단한 미세 조정 절차(일반적으로 BERT 위에 완전연결층 레이어 하나를 추가하고 몇 epoch 훈련)는 언어 추론, 의미론적 유사성, 질문 응답 등 다양한 작업에 대한 최소한의 작업 별 조정으로 최첨단 결과를 달성하는 것으로 나타났습니다. 특정 작업에서 잘 작동하는 것으로 보이는 사용자 지정 및 때로는 모호한 아키텍처를 구현하는 대신 BERT를 미세 조정하는 것이 더 나은(또는 최소한 동등한) 대안으로 보입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZEynC5F4u7Nb"
   },
   "source": [
    "\n",
    "### A Shift in NLP\n",
    "\n",
    "- 학습을 이전하는 이러한 변화는 몇 년 전 컴퓨터비전에서 일어난 것과 같은 변화와 유사합니다. 컴퓨터 비전 작업을 위한 우수한 딥러닝 네트워크를 만들려면 수백만 개의 매개 변수를 사용하고 훈련하는 데 비용이 많이들 수 있습니다. 연구원들은 딥 네트워크가 계층적 특징 표현(가장 낮은 층의 가장자리와 같은 단순한 특징, 더 높은 층에서는 점차 더 복잡한 특징)을 배우는 것을 발견했습니다. \n",
    "\n",
    "\n",
    "- 매번 새로운 네트워크를 처음부터 교육하는 대신 일반화 된 이미지 기능을 갖춘 훈련된 네트워크의 하위 계층을 복사하여 다른 작업을 수행하는 다른 네트워크에서 사용할 수 있습니다. 사전 훈련 된 딥 네트워크를 다운로드하고 새로운 작업을 위해 신속하게 재교육하거나 추가 계층을 추가하는 것이 곧 일반적인 관행이 되었습니다. 네트워크를 처음부터 교육하는 고가의 프로세스보다 훨씬 선호됩니다. \n",
    "\n",
    "\n",
    "- 많은 사람들에게 2018 년에 사전 훈련된 딥 언어 모델 (ELMO, BERT, ULMFIT, Open-GPT 등)의 도입은 컴퓨터 비전 분야에서 보았던 Transfer Learning 으로의 전환과 동일한 신호를 NLP 분야에 보냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RX_ZDhicpHkV"
   },
   "source": [
    "# 1. 환경 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSU7yERLP_66"
   },
   "source": [
    "## 1.1. Using Colab GPU for Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "jA0xxRs7Hwq3",
    "outputId": "224a17da-7962-4679-ccbc-eb6a12d92b7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-rc4\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "if device_name:\n",
    "    print(device_name)\n",
    "else:\n",
    "    print('GPU not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cqG7FzRVFEIv"
   },
   "source": [
    "토치가 GPU를 사용하려면 GPU를 장치로 식별하고 지정해야합니다. 나중에 훈련 루프에서 데이터를 장치에 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2kTWp0mKIoCx",
    "outputId": "048a92a4-020b-44cd-98ca-187f9e129dc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 사용 : Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU 사용 :\", torch.cuda.get_device_name())\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available, CPU 사용\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ElsnSNUridI"
   },
   "source": [
    "## 1.2. Hugging Face Library 설치\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_N2UDLevYWn"
   },
   "source": [
    "- 다음으로 Hugging Face의 [transformers](https://github.com/huggingface/transformers) 패키지를 설치하여 BERT 작업을 위한 pytorch 인터페이스를 제공합니다. (이 라이브러리에는 OpenAI의 GPT 및 GPT-2와 같은 다른 사전 훈련 된 언어 모델에 대한 인터페이스가 포함되어 있습니다.) \n",
    "\n",
    "\n",
    "- 우리는 pytorch 인터페이스를 선택했습니다. Tensorflow 는 높은 수준의 API 는 사용하기 쉽지만 어떻게 작동하는지에 대한 통찰력을 제공하지는 않고, 저수준 tensorflow 코드는 많은 세부 사항을 포함하지만 이해하기 어렵기 떄문입니다.\n",
    "\n",
    "\n",
    "- 현재 Hugging Face 라이브러리는 BERT 작업에 가장 널리 사용되는 강력한 pytorch 인터페이스 입니다. 라이브러리에는 다양한 사전 훈련 된 Transformer 모델을 지원할 뿐만 아니라 특정 작업에 적합한 이러한 모델의 사전 빌드된 수정 사항도 포함되어 있습니다. 예를 들어, 이 튜토리얼에서는`BertForSequenceClassification`을 사용합니다.\n",
    "\n",
    "\n",
    "- 이 라이브러리에는 토큰 분류, 질문 응답, 다음 문장 사전 결정 등을 위한 작업별 클래스도 포함되어 있습니다. 이러한 사전 빌드된 클래스를 사용하면 목적에 맞게 BERT를 수정하는 프로세스가 간소화됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "id": "dVD1KGPyK8cw",
    "outputId": "85a144a8-2f2d-4c92-e468-36f9bcccfc3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
      "\u001b[K     |████████████████████████████████| 573kB 2.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.13.1)\n",
      "Collecting tokenizers==0.5.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7MB 12.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 29.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 39.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: botocore<1.17.0,>=1.16.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.16.1)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.1->boto3->transformers) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.1->boto3->transformers) (0.15.2)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=4146a55e7ffa1d1f593480aada3f1b964cb4200755108d6db1ddd3c1581b2929\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
      "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxddqmruamSj"
   },
   "source": [
    "이 노트북의 코드는 실제로 huggingface의 [run_glue.py](https://github.com/huggingface/transformers/blob/master/examples/run_glue.py) 예제 스크립트의 단순화 된 버전입니다.\n",
    "\n",
    "`run_glue.py`는 실행하려는 GLUE 벤치 마크 작업과 사용하려는 사전 훈련 된 모델을 선택할 수있는 유용한 유틸리티입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "guw6ZNtaswKc"
   },
   "source": [
    "# 2. Loading CoLA Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_9ZKxKc04Btk"
   },
   "source": [
    "단일 문장 분류를 위해 [CoLA (Corpus of Linguistic Acceptability)](https://nyu-mll.github.io/CoLA/) 데이터 세트를 사용합니다. **문법적으로 정확하거나 틀린 것으로 표시**된 문장 세트입니다. 2018 년 5 월에 처음 출판되었으며 BERT와 같은 모델이 경쟁하는 **\"GLUE Benchmark\"**에 포함 된 테스트 중 하나입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4JrUHXms16cn"
   },
   "source": [
    "## 2.1. Download & Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ZNVW6xd0T0X"
   },
   "source": [
    "`wget` 패키지를 사용하여 데이터 셋을 Colab 인스턴스의 파일 시스템으로 다운로드."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "5m6AnuFv0QXQ",
    "outputId": "461ca343-abb0-4273-dbf2-f4e959aa5eb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=3e372e77d670ee383f99f9aa783d6e8d4060c934649aa4604324b85264e03f69\n",
      "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "08pO03Ff1BjI"
   },
   "source": [
    "dataset 은 GitHub 에서 repo 가능: https://nyu-mll.github.io/CoLA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "dkRVoub6jno0",
    "outputId": "c12d213e-7f8f-48e3-9699-dea545a65358"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  cola_public_1.1.zip\n",
      "   creating: cola_public/\n",
      "  inflating: cola_public/README      \n",
      "   creating: cola_public/tokenized/\n",
      "  inflating: cola_public/tokenized/in_domain_dev.tsv  \n",
      "  inflating: cola_public/tokenized/in_domain_train.tsv  \n",
      "  inflating: cola_public/tokenized/out_of_domain_dev.tsv  \n",
      "   creating: cola_public/raw/\n",
      "  inflating: cola_public/raw/in_domain_dev.tsv  \n",
      "  inflating: cola_public/raw/in_domain_train.tsv  \n",
      "  inflating: cola_public/raw/out_of_domain_dev.tsv  \n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import os\n",
    "\n",
    "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
    "\n",
    "if not os.path.exists('./cola_public_1.1.zip'):\n",
    "    wget.download(url, './cola_public_1.1.zip')\n",
    "\n",
    "if not os.path.exists('./cola_public'):\n",
    "    !unzip cola_public_1.1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQUy9Tat2EF_"
   },
   "source": [
    "## 2.2. Parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeyVCXT31EZQ"
   },
   "source": [
    "파일 이름에서 `tokenized` 및 `raw` 버전의 데이터를 모두 사용할 수 있음을 알 수 있습니다.\n",
    "\n",
    "**pre-trained BERT 를 적용하려면 BERT 모델에서 제공하는 tokenizer 를 사용해야하므로 `tokenized` version 의 data 는 사용할 수 없습니다.** 이는, \n",
    "\n",
    "\n",
    "(1) BERT 모델에 특정한 고정 어휘가 있고, \n",
    "\n",
    "\n",
    "(2) BERT tokenizer 가 out-of-vocabulary word 를 처리하는 특정 방법을 가지고 있기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MYWzeGSY2xh3"
   },
   "source": [
    "pandas 를 사용하여 트레이닝 세트를 구문 분석하고 해당 속성 및 데이터 포인트를 살펴 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "colab_type": "code",
    "id": "UIYHKq7Y2v5K",
    "outputId": "fc168d8a-12fd-4537-fe0d-e7ff28f7c934"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8551, 4)\n",
      "(527, 4)\n",
      "(516, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_source</th>\n",
       "      <th>label</th>\n",
       "      <th>label_notes</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6597</th>\n",
       "      <td>g_81</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>John offered, and Harry gave, Sally a Cadillac.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7224</th>\n",
       "      <td>sks13</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What the spy became was too friendly with his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3428</th>\n",
       "      <td>ks08</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A ball kicked the man.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4851</th>\n",
       "      <td>ks08</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>That Bill tried to discover which drawer Alice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>bc01</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>We love they.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7543</th>\n",
       "      <td>sks13</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John heard their criticism of each other.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8176</th>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The amoeba coughed and then it fainted.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>bc01</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John regards professors as strange and politic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2754</th>\n",
       "      <td>l-93</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gwen exchanged the dress for a shirt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000</th>\n",
       "      <td>sgww85</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John sang beautifully.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_source  ...                                           sentence\n",
       "6597            g_81  ...    John offered, and Harry gave, Sally a Cadillac.\n",
       "7224           sks13  ...  What the spy became was too friendly with his ...\n",
       "3428            ks08  ...                             A ball kicked the man.\n",
       "4851            ks08  ...  That Bill tried to discover which drawer Alice...\n",
       "770             bc01  ...                                      We love they.\n",
       "7543           sks13  ...          John heard their criticism of each other.\n",
       "8176            ad03  ...            The amoeba coughed and then it fainted.\n",
       "665             bc01  ...  John regards professors as strange and politic...\n",
       "2754            l-93  ...              Gwen exchanged the dress for a shirt.\n",
       "7000          sgww85  ...                             John sang beautifully.\n",
       "\n",
       "[10 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None,\n",
    "                             names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "df_test = pd.read_csv(\"./cola_public/raw/in_domain_dev.tsv\", delimiter='\\t', header=None,\n",
    "                             names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "df_val = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None,\n",
    "                             names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "print(df_val.shape)\n",
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfWzpPi92UAH"
   },
   "source": [
    "우리가 실제로 관심을 갖는 두 가지 속성은 `sentence` 와 `label` (\"acceptibility judgment\", 0=unacceptable, 1=acceptable) 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H_LpQfzCn9_o"
   },
   "source": [
    "문법적으로 허용되지 않는 것으로 표시된 5 개의 문장이 있습니다. 이 작업이 감정 분석과 같은 것보다 얼마나 어려운지 주목하십시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "-sPZCzQb5oHT",
    "outputId": "b3832760-76b1-48bf-d564-63e5ea0a2eb6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5483</th>\n",
       "      <td>It was so awful a picture as it first seemed.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>The defendant denied.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4067</th>\n",
       "      <td>That John coughs loves Bill.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Harry coughed us into a fit.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2201</th>\n",
       "      <td>Tessa sprained.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence  label\n",
       "5483  It was so awful a picture as it first seemed.      0\n",
       "3878                          The defendant denied.      0\n",
       "4067                   That John coughs loves Bill.      0\n",
       "26                     Harry coughed us into a fit.      0\n",
       "2201                                Tessa sprained.      0"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문법적으로 unacceptable\n",
    "\n",
    "df_train[df_train['label'] == 0].sample(5)[['sentence', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "RAQ8zp9s_G2x",
    "outputId": "147e0855-30d4-462b-98a7-e926d1234802"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6381</th>\n",
       "      <td>Almost any pilot could be flying this plane.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2868</th>\n",
       "      <td>That chisel carved the statue.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8409</th>\n",
       "      <td>The men chuckle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4519</th>\n",
       "      <td>You can do it, but you better not.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7860</th>\n",
       "      <td>Anson is incredibly difficult to please.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          sentence  label\n",
       "6381  Almost any pilot could be flying this plane.      1\n",
       "2868                That chisel carved the statue.      1\n",
       "8409                               The men chuckle      1\n",
       "4519            You can do it, but you better not.      1\n",
       "7860      Anson is incredibly difficult to please.      1"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문법적으로 acceptable\n",
    "\n",
    "df_train[df_train['label'] == 1].sample(5)[['sentence', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "WAk6jcIg_G20",
    "outputId": "90f69c1f-8e6b-4362-b92c-82961b6e14f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fbcd7bda1d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train['label'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4SMZ5T5Imhlx"
   },
   "source": [
    "\n",
    "\n",
    "훈련 세트의 문장과 레이블을 numpy ndarrays로 추출해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GuE5BqICAne2"
   },
   "outputs": [],
   "source": [
    "# Get the lists of sentences and their labels.\n",
    "train_sentences = df_train.sentence.values\n",
    "train_labels = df_train.label.values\n",
    "\n",
    "test_sentences = df_test.sentence.values\n",
    "test_labels = df_test.label.values\n",
    "\n",
    "val_sentences = df_val.sentence.values\n",
    "val_labels = df_val.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ex5O1eV-Pfct"
   },
   "source": [
    "# 3. Tokenization & Input Formatting\n",
    "\n",
    "데이터 세트를 BERT를 학습 할 수있는 형식으로 변환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-8kEDRvShcU5"
   },
   "source": [
    "## 3.1. BERT Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bWOPOyWghJp2"
   },
   "source": [
    "\n",
    "텍스트를 BERT에 제공하려면 토큰으로 분할 한 다음 tokenizer vocabulary 에서 해당 토큰을 색인에 매핑해야 합니다.\n",
    "\n",
    "토큰화는 BERT에 포함 된 토크나이저에 의해 수행되어야 합니다. 아래 셀에서 uncased version 을 이용하여 이를 다운로드합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "8e956671b68b4bd2a3115908aeb4b9cc",
      "e636274e9d8c4cecb07da4aff2deb5ca",
      "f9d120d0574c497bb69493c929917518",
      "261dee99ae0f43888729246ea00790e4",
      "9136ef5cc11a41439e3991ea0c46f3bf",
      "f9b6a7ba136a4829b14f875c1a2e2ba7",
      "af65fe74899446ef84256b4a45910c6b",
      "4c2fa4a84bee4514999642bc37b6ce01"
     ]
    },
    "colab_type": "code",
    "id": "IOQl4De86mal",
    "outputId": "450769ef-7221-4af7-b152-7349bf49a3b8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e956671b68b4bd2a3115908aeb4b9cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dFzmtleW6KmJ"
   },
   "source": [
    "결과를 보기 위해 토크 나이저를 한 문장에 적용 해 봅시다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "dLIbudgfh6F0",
    "outputId": "22be44a6-b2d8-44f7-8899-498fe1841c7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
      "Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
      "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', train_sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(train_sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_sentences[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WeNIc4auFUdF"
   },
   "source": [
    "실제로는 모든 문장을 변환 할 때,`tokenize`와`convert_tokens_to_ids`를 별도로 호출하지 않고`tokenize.encode` 함수를 사용하여 두 단계를 모두 처리합니다.\n",
    "\n",
    "그러려면 BERT 의 formatting requirement 를 알아야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "viKGCCh8izww"
   },
   "source": [
    "## 3.2. BERT 의 formatting requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yDcqNlvVhL5W"
   },
   "source": [
    "BERT 가 요구하는 input data formatting 은 다음을 수행해야 합니다.\n",
    "\n",
    "1. 각 문장의 시작과 끝에 특별한 토큰을 추가 \n",
    "\n",
    "\n",
    "2. 모든 문장을 하나의 일정한 길이로 padding 하거나 truncate  \n",
    "\n",
    "\n",
    "3. \"attetion mask\"를 사용하여 real token 과 padding token 을 명시 적으로 구별"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V6mceWWOjZnw"
   },
   "source": [
    "### Special Tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ykk0P9JiKtVe"
   },
   "source": [
    "\n",
    "**`[SEP]`**\n",
    "\n",
    "모든 문장의 끝에 특별한 `[SEP]` 을 추가해야 합니다.\n",
    "\n",
    "이 토큰은 BERT 에 두 개의 개별 문장이 주어지고 무언가를 결정하도록 하는 **two-sentence tasks** 를 위한 것 입니다. (예 : 문장 A의 질문에 대한 답변을 문장 B에서 찾을 수 있는가 ?).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86C9objaKu8f"
   },
   "source": [
    "**`[CLS]`**\n",
    "\n",
    "분류 작업을 위해서는 모든 문장의 시작 부분에 특수한 `[CLS]` 토큰을 붙여야합니다.\n",
    "\n",
    "이 토큰에는 특별한 의미가 있습니다. BERT는 12 개의 트랜스포머 레이어로 구성됩니다. 각 Transformer 는 토큰 임베딩 목록을 가져와 출력에 동일한 수의 임베딩을 생성합니다 (물론 feature 값은 변경됨).\n",
    "\n",
    "<img src=\"http://www.mccormickml.com/assets/BERT/padding_and_mask.png\" width=\"600\">\n",
    "(Illustration of CLS token purpose)\n",
    "\n",
    "최종 (12 번째) Transformer 의 출력에서 *classifier 가 ([CLS] 토큰에 해당하는) 첫 번째 embedding 만 사용 합니다*.\n",
    "> \"모든 sequence 의 첫 번째 토큰은 항상 special classification token (`[CLS]`)입니다. 이 토큰에 해당하는 마지막 hidden layer state 가 classification task 의 집약된 sequence 표현으로 사용됩니다.\"(from the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
    "\n",
    "또한 BERT는 분류를 위해 이 [CLS] 토큰만 사용하도록 훈련되었으므로 이 단일 768-value 임베딩 벡터로 분류 단계에 필요한 모든 것을 인코딩하도록 모델이 동기부여 되었음을 알 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u51v0kFxeteu"
   },
   "source": [
    "### Sentence Length & Attention Mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qPNuwqZVK3T6"
   },
   "source": [
    "다양한 길이의 데이터 세트의 문장처리:\n",
    "\n",
    "BERT에는 두 가지 제약 조건이 있습니다.\n",
    "1. 모든 문장은 단일 고정 길이로 padding or truncated.\n",
    "2. 최대 문장 길이는 512 토큰입니다.\n",
    "\n",
    "패딩은 BERT vocabulary 에서 index 0 인 특별한 `[PAD]` 토큰으로 수행됩니다. 아래 그림은 \"MAX_LEN\" 8 인 경우 padding 방법을 보여줍니다.\n",
    "<img src=\"http://www.mccormickml.com/assets/BERT/padding_and_mask.png\" width=\"600\">\n",
    "\n",
    "\"Attention Mask\"는 1과 0으로 구성된 배열로, 어떤 토큰이 패딩되고 있지 않은지를 나타냅니다.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l6w8elb-58GJ"
   },
   "source": [
    "## 3.2. Sentences to IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1M296yz577fV"
   },
   "source": [
    "`tokenizer.encode` 함수는 여러 단계를 한번에 수행합니다.\n",
    "1. 문장을 토큰으로 분리.\n",
    "2. 특별한`[CLS]`및`[SEP]`토큰을 추가\n",
    "3. 토큰을 ID에 맵핑\n",
    "\n",
    "단, truncate 는 해주지만 패딩은 처리하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VRDsmEb1_G3Q"
   },
   "outputs": [],
   "source": [
    "def encoding(sentences):\n",
    "    input_ids = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode(sent, add_special_tokens=True)\n",
    "        input_ids.append(encoded_sent)\n",
    "        \n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "colab_type": "code",
    "id": "QAL5xIzRFMFg",
    "outputId": "4567626d-84d3-4e33-98e4-79778e9f7157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8551\n",
      "Our friends won't buy this analysis, let alone the next one we propose.\n",
      "[101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102]\n",
      "[CLS] our friends won't buy this analysis, let alone the next one we propose. [SEP]\n",
      "\n",
      "527\n",
      "The sailors rode the breeze clear of the rocks.\n",
      "[101, 1996, 11279, 8469, 1996, 9478, 3154, 1997, 1996, 5749, 1012, 102]\n",
      "[CLS] the sailors rode the breeze clear of the rocks. [SEP]\n",
      "\n",
      "516\n",
      "Somebody just left - guess who.\n",
      "[101, 8307, 2074, 2187, 1011, 3984, 2040, 1012, 102]\n",
      "[CLS] somebody just left - guess who. [SEP]\n"
     ]
    }
   ],
   "source": [
    "train_ids = encoding(train_sentences)\n",
    "test_ids =  encoding(test_sentences)\n",
    "val_ids =   encoding(val_sentences)\n",
    "\n",
    "print(len(train_ids))\n",
    "print(train_sentences[0])\n",
    "print(train_ids[0])\n",
    "print(tokenizer.decode(train_ids[0]))\n",
    "print()\n",
    "print(len(test_ids))\n",
    "print(test_sentences[0])\n",
    "print(test_ids[0])\n",
    "print(tokenizer.decode(test_ids[0]))\n",
    "print()\n",
    "print(len(val_ids))\n",
    "print(val_sentences[0])\n",
    "print(val_ids[0])\n",
    "print(tokenizer.decode(val_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WhwCKszh6ych"
   },
   "source": [
    "## 3.3. Padding & Truncating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xytsw1oIfnX0"
   },
   "source": [
    "시퀀스가 모두 동일한 길이 `MAX_LEN` 이 되도록 pad and truncate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JhUZO9vc_l6T",
    "outputId": "ab8bccfd-e047-4399-a52c-14abab29ab04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  47\n"
     ]
    }
   ],
   "source": [
    "print('Max sentence length: ', max([len(sen) for sen in train_ids]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hp-54FcQ_p3h"
   },
   "source": [
    "test 및 validation set 의 최대 크기를 모르므로 max length 47 보다 약간 큰 MAX_LEN = 64를 선택하고 패딩을 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "colab_type": "code",
    "id": "W--bPXmkGmAj",
    "outputId": "7cb9b72e-187c-400a-8d27-f877ca9205f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "[  101  2256  2814  2180  1005  1056  4965  2023  4106  1010  2292  2894\n",
      "  1996  2279  2028  2057 16599  1012   102     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "[  101  1996 11279  8469  1996  9478  3154  1997  1996  5749  1012   102\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "[ 101 8307 2074 2187 1011 3984 2040 1012  102    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 64\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "train_sequences = pad_sequences(train_ids, maxlen=MAX_LEN, truncating='post', padding='post')\n",
    "test_sequences = pad_sequences(test_ids, maxlen=MAX_LEN, truncating='post', padding='post')\n",
    "val_sequences = pad_sequences(val_ids, maxlen=MAX_LEN, truncating='post', padding='post')\n",
    "\n",
    "print(train_sequences[0])\n",
    "print(test_sequences[0])\n",
    "print(val_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kDs-MYtYH8sL"
   },
   "source": [
    "## 3.4. Attention Masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KhGulL1pExCT"
   },
   "source": [
    "BERT vocabulary 는 ID 0 을 사용하지 않으므로 토큰 ID가 0 이면 padding 이고 그렇지 않으면 실제 토큰입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kq0j-1dD_G3d"
   },
   "outputs": [],
   "source": [
    "def masking(sequences):\n",
    "    attention_masks = []\n",
    "    \n",
    "    for sent in sequences:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "    return attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "egJLDslzJTMa",
    "outputId": "27eb2dbe-b5f6-4225-abb5-4363d92e8983"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "train_masks = masking(train_sequences)\n",
    "test_masks = masking(test_sequences)\n",
    "val_masks = masking(val_sequences)\n",
    "\n",
    "print(train_masks[0])\n",
    "print(test_masks[0])\n",
    "print(val_masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7LzSbTqW9_BR"
   },
   "source": [
    "## 3.5. Converting to PyTorch Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p1uXczp-Je4"
   },
   "source": [
    "우리 모델은 numpy.ndarrays 대신 PyTorch 텐서가 필요하므로 모든 데이터 세트 변수를 변환\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0RMfP9syLtJT"
   },
   "outputs": [],
   "source": [
    "train_sequences = torch.tensor(train_sequences).to(torch.int64)\n",
    "test_sequences  = torch.tensor(test_sequences ).to(torch.int64)\n",
    "val_sequences  = torch.tensor(val_sequences ).to(torch.int64)\n",
    "\n",
    "train_labels = torch.tensor(train_labels).to(torch.int64)\n",
    "test_labels  = torch.tensor(test_labels).to(torch.int64)\n",
    "val_labels  = torch.tensor(val_labels).to(torch.int64)\n",
    "\n",
    "train_masks  = torch.tensor(train_masks ).to(torch.int64)\n",
    "test_masks  = torch.tensor(test_masks).to(torch.int64)\n",
    "val_masks  = torch.tensor(val_masks).to(torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dD9i6Z2pG-sN"
   },
   "source": [
    "또한 토치 DataLoader 클래스를 사용하여 데이터 세트에 대한 반복자를 만듭니다. 이는 for 루프와 달리 반복자와 함께 전체 데이터 세트를 메모리에 로드 할 필요가 없기 때문에  훈련 중에 메모리를 절약하는 데 도움이됩니다.\n",
    "\n",
    "### 특정 task 에서 BERT를 fine-tuning 할 때 배치 크기가 16 또는 32 로 하라고 recommend 하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FiKARlFgMWCD"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# training set 에 대한 DataLoader 생성\n",
    "train_data = TensorDataset(train_sequences, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# validation set 에 대한 DataLoader 생성\n",
    "test_data = TensorDataset(test_sequences, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_sequences, val_masks, val_labels)\n",
    "val_sampler = RandomSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8bwa6Rts-02-"
   },
   "source": [
    "# 4. Train Our Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3xYQ3iLO08SX"
   },
   "source": [
    "입력 데이터가 올바르게 포맷 되었으므로 BERT를 미세 조정할 때입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6TKgyUzPIQc"
   },
   "source": [
    "## 4.1. BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1sjzRT1V0zwm"
   },
   "source": [
    "이 task 에서는 분류를 위한 출력을 제공할 수 있도록 pre-trained BERT 모델을 먼저 수정한 후 엔드-투-엔드로 전체 모델이 우리 task 에 적합해질 때까지 데이터 집합에 대한 모델 training 을 계속해야 합니다.\n",
    "\n",
    "다행히도 huggingface pytorch 설치에는 다양한 NLP 작업을 위해 설계된 일련의 인터페이스가 포함되어 있습니다. 이러한 인터페이스는 모두 하나의 훈련된 BERT 모델을 기반으로 구축되었지만 각각 특정 NLP 작업을 수용하도록 설계된 서로 다른 top layer 및 출력 유형이 있습니다.\n",
    "\n",
    "fine-tuning 을 위해 제공되는 현재의 class list 입니다.:\n",
    "* BertModel\n",
    "* BertForPreTraining\n",
    "* BertForMaskedLM\n",
    "* BertForNextSentencePrediction\n",
    "* **BertForSequenceClassification** - 이 tutorial 에서 사용 예정\n",
    "* BertForTokenClassification\n",
    "* BertForQuestionAnswering\n",
    "\n",
    "documentation 위치 [here](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html) 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BXYitPoE-cjH"
   },
   "source": [
    "\n",
    "\n",
    "우리가 사용할 것은 [BertForSequenceClassification](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification)입니다.  \n",
    "\n",
    "이것은 sentence classifier 로 사용할 분류를 위한 단일 linear layer 가 추가 된 통상적 BERT 모델입니다. 입력 데이터를 공급할 때 사전 훈련 된 전체 BERT 모델과 추가된 훈련되지 않은 classification layer 가 특정 task 에 대해 훈련됩니다.\n",
    "\n",
    "For sequence pairs:\n",
    "\n",
    "    tokens:         [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "\n",
    "    token_type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "\n",
    "For single sequences:\n",
    "\n",
    "    tokens:         [CLS] the dog is hairy . [SEP]\n",
    "\n",
    "    token_type_ids:   0   0   0   0  0     0   0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WnQW9E-bBCRt"
   },
   "source": [
    "사용 가능한 몇 가지 사전 훈련 된 BERT 모델이 있습니다. \"bert-base-uncased\"는 소문자만 있는 version 이고 두 버전 중 더 작은 버전 (\"base\"대 \"large\")을 의미합니다.\n",
    "\n",
    "`from_pretrained` 문서 참조 [here](https://huggingface.co/transformers/v2.2.0/main_classes/model.html#transformers.PreTrainedModel.from_pretrained), 추가 parameter [here](https://huggingface.co/transformers/v2.2.0/main_classes/configuration.html#transformers.PretrainedConfig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eliaCzkoR3M9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=2,   # binary,  >= 3 multi-class task\n",
    "    output_attentions=False,                   # model 이 attention weights 를 반환할지 여부\n",
    "    output_hidden_states=False              # model 이 모든 hidden-state 를 반환할지 여부\n",
    ")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0Jv6c7-HHDW"
   },
   "source": [
    "참고삼아 아래의 names, dimensions of the weights 출력.\n",
    "\n",
    "1. The embedding layer.\n",
    "2. The first of the twelve transformers.\n",
    "3. The output layer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "colab_type": "code",
    "id": "tsyQUS7rTpRZ",
    "outputId": "37d7fcce-38bf-4d36-8a64-daccef614432"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRWT-D4U_Pvx"
   },
   "source": [
    "## 4.2. Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8o-VEBobKwHk"
   },
   "source": [
    "이제 모델이 로드 되었으므로 저장된 모델 내에서 training hyperparameters 를 가져와야합니다.\n",
    "\n",
    "fine-tuning 에 다음 값들이 recommend 됩니다. \n",
    "- 배치 크기 : 16, 32 (DataLoader 를 만들 때 32 를 선택).\n",
    "- Learning rate(Adam) : 5e-5, 3e-5, 2e-5 (2e-5 사용)\n",
    "- 에포크 수 : 2, 3, 4 (4를 사용).\n",
    "\n",
    "The epsilon parameter `eps = 1e-8` zero divide 방지를 위한 매우 작은 숫자임. (from [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n",
    "\n",
    "AdamW(Adam algorithm with weight decay fix) optimizer 는 다음 참조 `run_glue.py` [here](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "daJaQ6k9Xokg"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z4VN7mpNBeC6"
   },
   "source": [
    "* get_linear_schedule_with_warmup  \n",
    " Create a schedule with a learning rate that decreases linearly after\n",
    "linearly increasing during a warmup period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "tuRl1po3ZOsh",
    "outputId": "fb1d02d9-6dcc-411b-afdc-9029d022eb6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1072\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 4\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "print(total_steps)\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                num_warmup_steps=0,  # Default value in run_glue.py\n",
    "                                num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RqfmWwUR_Sox"
   },
   "source": [
    "## 4.3. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pE5B99H5H2-W"
   },
   "source": [
    "accuracy 계산을 위한 helper function 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hiE3rYe2avMy"
   },
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KNhRtWPXH9C3"
   },
   "source": [
    "경과시간 formatting 을 위한 helper function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpt6tR83keZD"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uLXYq0JFOhvd"
   },
   "source": [
    "- model.eval() : 모든 layer 가 eval mode 로 동작. batchnorm, dropout layers 가 training mode 아닌 eval mode 로 동작.\n",
    "- torch.no_grad() : autograd engine 을 deactivate 하여 memory 절약 및 speed 향상."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6J-FYdx6nFE_",
    "outputId": "b7887afa-c593-437f-d593-5c0378871b12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of    268.    Elapsed: 0:00:09.\n",
      "  Batch    80  of    268.    Elapsed: 0:00:17.\n",
      "  Batch   120  of    268.    Elapsed: 0:00:25.\n",
      "  Batch   160  of    268.    Elapsed: 0:00:33.\n",
      "  Batch   200  of    268.    Elapsed: 0:00:42.\n",
      "  Batch   240  of    268.    Elapsed: 0:00:50.\n",
      "\n",
      "  Average training loss: 0.49\n",
      "  Training epcoh took: 0:00:55\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.81\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    268.    Elapsed: 0:00:08.\n",
      "  Batch    80  of    268.    Elapsed: 0:00:16.\n",
      "  Batch   120  of    268.    Elapsed: 0:00:25.\n",
      "  Batch   160  of    268.    Elapsed: 0:00:33.\n",
      "  Batch   200  of    268.    Elapsed: 0:00:41.\n",
      "  Batch   240  of    268.    Elapsed: 0:00:49.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epcoh took: 0:00:55\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.86\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    268.    Elapsed: 0:00:08.\n",
      "  Batch    80  of    268.    Elapsed: 0:00:17.\n",
      "  Batch   120  of    268.    Elapsed: 0:00:25.\n",
      "  Batch   160  of    268.    Elapsed: 0:00:33.\n",
      "  Batch   200  of    268.    Elapsed: 0:00:41.\n",
      "  Batch   240  of    268.    Elapsed: 0:00:50.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epcoh took: 0:00:55\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.84\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    268.    Elapsed: 0:00:08.\n",
      "  Batch    80  of    268.    Elapsed: 0:00:17.\n",
      "  Batch   120  of    268.    Elapsed: 0:00:25.\n",
      "  Batch   160  of    268.    Elapsed: 0:00:33.\n",
      "  Batch   200  of    268.    Elapsed: 0:00:41.\n",
      "  Batch   240  of    268.    Elapsed: 0:00:49.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epcoh took: 0:00:55\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.84\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs): \n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    st = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - st)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'\n",
    "                        .format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, # sentence 구분 [0,0,0,0,1,1,1,1]\n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        \n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple (loss, logits, hidden_states, attentions)\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # training data 의 평균 loss 계산\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # learning curve 시각화를 위해 loss value 저장.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - st)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # 각 training epoch 이 끝나면, Test set 에 대한 performance 측정.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    st = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in test_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "            \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - st)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-G03mmwH3aI"
   },
   "source": [
    "### 전체 batche 에 대한 training loss 를 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "colab_type": "code",
    "id": "68xreA9JAmG5",
    "outputId": "2f29f2df-da46-4719-d1a3-1ad606022d40"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# seaborn plot style\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(loss_values, 'b-o')\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mkyubuJSOzg3"
   },
   "source": [
    "# 5. Performance On Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DosV94BYIYxg"
   },
   "source": [
    "* 이제 훈련 세트에서와 마찬가지로 홀드아웃 데이터 세트를 로드하고 입력을 준비합니다. 그런 다음 [Matthew의 상관 계수](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)를 사용하여 예측을 평가합니다. 이는 더 폭넓은 NLP 커뮤니티에서 CoLA 성능 평가에 사용하는 메트릭이기 때문입니다. \n",
    "* +1 : 완전 예측, 0 : 평균 랜덤 예측, -1 : 역예측\n",
    "$$MCC = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tg42jJqqM68F"
   },
   "source": [
    "### 5.1. Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xWe0_JW21MyV"
   },
   "source": [
    "훈련 데이터에 테스트 데이터 세트를 준비하기 위해했던 것과 동일한 단계를 모두 적용해야합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16lctEOyNFik"
   },
   "source": [
    "## 5.2. Evaluate on Test Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rhR99IISNMg9"
   },
   "source": [
    "테스트 세트가 준비되면 Fine-tuning 된 모델을 적용하여 테스트 세트에 대한 예측을 생성 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "Hba10sXR7Xi6",
    "outputId": "66c54ae4-fa9e-469a-97bb-3b2af05bb2df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 516 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "print('Predicting labels for {:,} test sentences...'.format(len(val_sentences)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "for batch in val_dataloader:\n",
    "\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-5jscIM8R4Gv"
   },
   "source": [
    "CoLA 벤치 마크의 정확도는 \"[매튜스 상관 계수](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)\"(MCC)를 사용하여 측정됩니다.\n",
    "\n",
    "클래스가 불균형하기 때문에 MCC를 사용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hWcy0X1hirdx",
    "outputId": "f9e37ebb-c5ea-45b1-b717-62ddd8956345"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples:354 of 516 (68.60%)\n"
     ]
    }
   ],
   "source": [
    "print(f'Positive samples:{df_val.label.sum()} of {len(df_val.label)} ({df_val.label.sum() / len(df_val.label) * 100.0:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "cRaZQ4XC7kLs",
    "outputId": "db72da29-ae86-40e8-c058-aafaadf46aad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Matthews Corr. Coef. for each batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "  \n",
    "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "  # in to a list of 0s and 1s.\n",
    "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "  \n",
    "  # Calculate and store the coef for this batch.  \n",
    "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "  matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IUM0UA1qJaVB"
   },
   "source": [
    "최종 점수는 전체 테스트 세트를 기준으로하지만, 개별 배치의 점수를보고 배치 간 메트릭의 변동성을 파악하십시오.\n",
    "\n",
    "각 배치에는 (516 % 32) = 4 개의 테스트 문장 만있는 마지막 배치를 제외하고 32 개의 문장이 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "colab_type": "code",
    "id": "xytAr_C48wnu",
    "outputId": "8bfa3e54-59ce-4f50-88ec-62104105df25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.39405520311955033,\n",
       " 0.7679476477883045,\n",
       " 0.14907119849998599,\n",
       " 0.45693676673168776,\n",
       " 0.6695340634119862,\n",
       " 0.49382916465843113,\n",
       " 0.6078306738548308,\n",
       " 0.5141671091113803,\n",
       " 0.6454972243679028,\n",
       " 0.2809003238667948,\n",
       " 0.5691908489504616,\n",
       " 0.5204956780951701,\n",
       " 0.5202453946507077,\n",
       " 0.589872830909857,\n",
       " 0.5962847939999439,\n",
       " 0.458682472293863,\n",
       " 0.0]"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "oCYZa1lQ8Jn8",
    "outputId": "1ce2c476-316b-425b-f6e5-32c892e0036c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.513\n"
     ]
    }
   ],
   "source": [
    "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jXx0jPc4HUfZ"
   },
   "source": [
    "약 30 분 만에 하이퍼 파라미터 튜닝 (학습 속도, 에포크 (epoch), 배치 크기, ADAM 속성 등 조정)을 수행하지 않아도 좋은 점수를 얻을 수 있었습니다.\n",
    "\n",
    "You can also look at the official leaderboard [here](https://gluebenchmark.com/leaderboard/submission/zlssuBTm5XRs0aSKbFYGVIVdvbj1/-LhijX9VVmvJcvzKymxy). \n",
    "\n",
    "(작은 데이터 세트 크기로 인해?) 정확도는 다른 random seed 에 따라 크게 다를 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUmsUOIv8EUO"
   },
   "source": [
    "# Appendix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q2079Qyn8Mt8"
   },
   "source": [
    "## A1. Saving & Loading Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "6ulTWaOr8QNY",
    "outputId": "5cbd9daa-8c62-4dcc-ea41-4d9b1c930469"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/vocab.txt',\n",
       " './model_save/special_tokens_map.json',\n",
       " './model_save/added_tokens.json')"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_dir = './model_save/'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(f\"Saving model to {output_dir}\")\n",
    "\n",
    "# `save_pretrained()` 을 이용하여 trained model, configuration, tokenizer 저장\n",
    "# `from_pretrained()` 를 이용하여 reload\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z-tjHkR7lc1I"
   },
   "source": [
    "파일 크기 check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "mqMzI3VTCZo5",
    "outputId": "4847c890-28c5-4fce-f584-6019fad30df3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 427960K\n",
      "-rw-r--r-- 1 root root      2K Apr 30 12:24 config.json\n",
      "-rw-r--r-- 1 root root 427719K Apr 30 12:24 pytorch_model.bin\n",
      "-rw-r--r-- 1 root root      1K Apr 30 12:24 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root      1K Apr 30 12:24 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root    227K Apr 30 12:24 vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -l --block-size=K ./model_save/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fr_bt2rFlgDn"
   },
   "source": [
    "model weights 는 약 418 megabyte 의 크기이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-WUFUIQ8Cu8D",
    "outputId": "9cdb03ee-67ee-4312-fedf-9858bc7f638d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 418M Apr 30 12:24 ./model_save/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -l --block-size=M ./model_save/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WV6T7fb1C-iT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "145_BERT_Fine_Tuning_Sentence_Classification_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "261dee99ae0f43888729246ea00790e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c2fa4a84bee4514999642bc37b6ce01",
      "placeholder": "​",
      "style": "IPY_MODEL_af65fe74899446ef84256b4a45910c6b",
      "value": " 232k/232k [00:00&lt;00:00, 307kB/s]"
     }
    },
    "4c2fa4a84bee4514999642bc37b6ce01": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e956671b68b4bd2a3115908aeb4b9cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f9d120d0574c497bb69493c929917518",
       "IPY_MODEL_261dee99ae0f43888729246ea00790e4"
      ],
      "layout": "IPY_MODEL_e636274e9d8c4cecb07da4aff2deb5ca"
     }
    },
    "9136ef5cc11a41439e3991ea0c46f3bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "af65fe74899446ef84256b4a45910c6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e636274e9d8c4cecb07da4aff2deb5ca": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9b6a7ba136a4829b14f875c1a2e2ba7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9d120d0574c497bb69493c929917518": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9b6a7ba136a4829b14f875c1a2e2ba7",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9136ef5cc11a41439e3991ea0c46f3bf",
      "value": 231508
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
