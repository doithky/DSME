{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "195_Generating_LSTM_Sentences_YoungPrince_Korean_next_word.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZyxyanlZ5aJp"
      },
      "source": [
        "# 195. Keras API 와 LSTM 을 이용한 한글 어린왕자 문장 생성기\n",
        "\n",
        "- next word 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BOwsuGQQY9OL",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow.keras.regularizers as regularizers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZimSMFiHv_pL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "008a6aed-9d01-47a5-cfa6-1c2a9f24cc1a"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('alice.txt', \"https://github.com/ironmanciti/DSME/raw/master/datasets/%EC%96%B4%EB%A6%B0%EC%99%95%EC%9E%90-dmsah10.txt\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/ironmanciti/DSME/raw/master/datasets/%EC%96%B4%EB%A6%B0%EC%99%95%EC%9E%90-dmsah10.txt\n",
            "81920/75644 [================================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n19R9Jpy5aJu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "81a08096-db6b-46b5-ba98-8f0ce94676ce"
      },
      "source": [
        "r = open(path_to_file, 'r', encoding='cp949')\n",
        "\n",
        "texts = r.readlines()\n",
        "lines = []\n",
        "\n",
        "for line in texts:\n",
        "    line = line.strip().lower()\n",
        "    if len(line) == 0:\n",
        "        continue\n",
        "    lines.append(line)\n",
        "\n",
        "text = \" \".join(lines)\n",
        "text[:1000]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'여섯 살 적에 나는 \"체험한 이야기\"라는 제목의, 원시림에 관한 책에서 기막힌 그림 하나를 본 적이 있다. 맹수를 집어삼키고 있는 보아 구렁이 그림이었다. 위의 그림은 그것을 옮겨 그린 것이다. 그 책에는 이렇게 씌어 있었다. \"보아 구렁이는 먹이를 씹지도 않고 통째로 집어삼킨다.그리고는 꼼짝도 하지 못하고 여섯 달 동안 잠을 자면서 그것을 소화시킨다.\" 나는 그래서 밀림 속에서의 모험에 대해 한참 생각해 보고 난 끝에 색연필을 가지고 내 나름대로 내 생애 첫번째 그림을 그려보았다. 나의 그림 제 1호였다. 그것은 이런 그림이었다. 나는 그 걸작품을 어른들에게 보여 주면서내 그림이 무섭지 않느냐고 물었다. 그들은 \"모자가 뭐가 무섭다는 거니?\" 하고 대답했다. 내 그림은 모자를 그린 게 아니었다. 그것은 코끼리를 소화시키고 있는 보아 구렁이었다. 그래서 나는 어른들이 알아볼 수 있도록 보아 구렁이의 속을 그렸다. 어른들은 언제나 설명을 해주어야만 한다. 나의 그림 제 2호는 이러했다. 어른들은 속이 보이거나 보이지 않거나 하는 보아 구렁이의 그림들은 집어치우고 차라리 지리, 역사, 계산, 그리고 문법 쪽에 관심을 가져보는 게 좋을 것이라고 충고해 주었다. 그래서 나는 여섯 살 적에 화가라는 멋진 직업을 포기해 버렸다.내 그림제 1호와 제 2호가 성공을 거두지 못한 데 낙심해 버렸던 것이다. 어른들은언제나 스스로는 아무것도 이해하지 못한다.자꾸자꾸 설명을 해주어야 하니 맥빠지는 노릇이 아닐 수 없다. 그래서 다른 직업을 선택하지 않을 수 없게 된 나는 비행기 조종하는 법을배웠다.세계의 여기저기 거의 안 가본 데 없이 나는 날아다녔다.그러니지리는 정말로 많은 도움을 준 셈이었다.한번 슬쩍 보고도 중국과 애리조나를 나는 구별할 수 있었던 것이다.그것은 밤에 길을 잃었을 때 아주 유용한 일이다. 나는 그리하여 일생 동안 수없이 많은 점잖은 사람들과수많은 접촉을 가져왔다.어른들 틈에서 많이 살아온 것이다.나는 가까이서 그들을 볼 수있었다. 그렇다고 해서 그들에 대한 내'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7SZhigNh5aJx",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "corpus = re.split('[,.]', text)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zk-Kn9785aJ0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "984f79c4-d82a-48a5-e5a0-7a129989a34e"
      },
      "source": [
        "corpus[:10]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['여섯 살 적에 나는 \"체험한 이야기\"라는 제목의',\n",
              " ' 원시림에 관한 책에서 기막힌 그림 하나를 본 적이 있다',\n",
              " ' 맹수를 집어삼키고 있는 보아 구렁이 그림이었다',\n",
              " ' 위의 그림은 그것을 옮겨 그린 것이다',\n",
              " ' 그 책에는 이렇게 씌어 있었다',\n",
              " ' \"보아 구렁이는 먹이를 씹지도 않고 통째로 집어삼킨다',\n",
              " '그리고는 꼼짝도 하지 못하고 여섯 달 동안 잠을 자면서 그것을 소화시킨다',\n",
              " '\" 나는 그래서 밀림 속에서의 모험에 대해 한참 생각해 보고 난 끝에 색연필을 가지고 내 나름대로 내 생애 첫번째 그림을 그려보았다',\n",
              " ' 나의 그림 제 1호였다',\n",
              " ' 그것은 이런 그림이었다']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B0aXEN_15aJ2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9840d75c-0c41-4ff0-94f0-1414a0bdaa36"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "print(total_words)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rn-AIOff5aJ4",
        "colab": {}
      },
      "source": [
        "# create input sequences using list of tokens\n",
        "input_sequences = []\n",
        "\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tsplC63C5aJ6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "83be7e98-f53c-436c-f8e6-471e99a8af7f"
      },
      "source": [
        "print(len(input_sequences))\n",
        "input_sequences[:10]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9365\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[142, 245],\n",
              " [142, 245, 371],\n",
              " [142, 245, 371, 3],\n",
              " [142, 245, 371, 3, 1322],\n",
              " [142, 245, 371, 3, 1322, 739],\n",
              " [142, 245, 371, 3, 1322, 739, 372],\n",
              " [142, 245, 371, 3, 1322, 739, 372, 1323],\n",
              " [1324, 1325],\n",
              " [1324, 1325, 1326],\n",
              " [1324, 1325, 1326, 1327]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-kSzQM4p5aJ8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "e1aa59bd-ff61-47fb-c166-66a4469ee06f"
      },
      "source": [
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "input_sequences"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,    0,  142,  245],\n",
              "       [   0,    0,    0, ...,  142,  245,  371],\n",
              "       [   0,    0,    0, ...,  245,  371,    3],\n",
              "       ...,\n",
              "       [   0,    0,    0, ..., 4591, 1139, 4592],\n",
              "       [   0,    0,    0, ..., 1139, 4592, 4593],\n",
              "       [   0,    0,    0, ..., 4592, 4593,  525]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yU3N25jx5aJ-",
        "colab": {}
      },
      "source": [
        "text_dataset = tf.data.Dataset.from_tensor_slices(input_sequences)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YWPQPgs05aKB",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[-1]\n",
        "    return input_text, target_text"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PlE-4oP55aKF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "970a470d-1bb1-4ada-9a5f-66b5d0581c7f"
      },
      "source": [
        "dataset = text_dataset.map(split_input_target).batch(256, drop_remainder=True)\n",
        "\n",
        "for input, target in dataset.take(1):\n",
        "  print(input)\n",
        "  print()\n",
        "  print(target)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[   0    0    0 ...    0    0  142]\n",
            " [   0    0    0 ...    0  142  245]\n",
            " [   0    0    0 ...  142  245  371]\n",
            " ...\n",
            " [   0    0    0 ...  128  189 1418]\n",
            " [   0    0    0 ...  189 1418 1419]\n",
            " [   0    0    0 ...    0    0   23]], shape=(256, 51), dtype=int32)\n",
            "\n",
            "tf.Tensor(\n",
            "[ 245  371    3 1322  739  372 1323 1325 1326 1327  207  740   87  126\n",
            "  143 1329   10  113  373  741  298  144  742  246   13  743   61 1330\n",
            "   20  744 1331 1332  186 1333 1334  745   43  493  142 1335  299  247\n",
            " 1336  144 1337   17 1338 1339 1340  114  374  300  167   14 1341 1342\n",
            "   91   11 1343   11 1344  746  145 1345  207  248 1346  168  741    2\n",
            " 1347  747  748 1348 1349  749 1350   33  750  751 1351  494   21   28\n",
            "  298  375  246   26  376  752 1352   10  113 1353    3  495  496    9\n",
            " 1354  113  497 1355  249  127  498 1356  147  207  248 1357 1358  301\n",
            "  499   81  753   27  113  497 1359 1360 1361  754 1362 1363 1364  757\n",
            "   26  758  759 1365  377    3  142  245  371 1366  500  501  502  250\n",
            " 1367 1368  248 1369 1370 1371  503  251 1372 1373   13 1375  106  504\n",
            "  760  498  761  762 1377 1378 1379    9  252   39  501 1380  169    9\n",
            "  302  115    3 1381 1382 1383 1385  763  253 1386  251  505    3 1387\n",
            "  208   88 1389  148 1390  764 1391 1392 1393    3  765    9  187   13\n",
            "  506  149 1394  116   34 1395  766  254 1396  299 1397   88 1398 1399\n",
            " 1400 1401 1402  768 1403   13 1404  209  210 1405  255 1407  303   11\n",
            "  188 1408   63 1409  304  507  769 1410  770 1411 1412  379 1413   76\n",
            "  207  248 1414   91    2  507 1415 1416   57   29 1417  256  128  189\n",
            " 1418 1419   13 1420], shape=(256,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PRnDnCW-Z7qv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "df2b5e1d-8837-49fd-a86f-3ca1183ebb8a"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100))\n",
        "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(16))\n",
        "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 100)         459400    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 64)          34048     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 64)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 16)                5184      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2297)              39049     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4594)              10557012  \n",
            "=================================================================\n",
            "Total params: 11,094,693\n",
            "Trainable params: 11,094,693\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AIg2f1HBxqof",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6911b118-b8ad-460d-c4a2-6d372e76b8b7"
      },
      "source": [
        " %%time\n",
        " history = model.fit(dataset, epochs=200, verbose=1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 8.4071 - accuracy: 0.0088\n",
            "Epoch 2/200\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 7.8029 - accuracy: 0.0110\n",
            "Epoch 3/200\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 7.6893 - accuracy: 0.0117\n",
            "Epoch 4/200\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 7.5257 - accuracy: 0.0105\n",
            "Epoch 5/200\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 7.4628 - accuracy: 0.0091\n",
            "Epoch 6/200\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 7.4117 - accuracy: 0.0101\n",
            "Epoch 7/200\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 7.3437 - accuracy: 0.0084\n",
            "Epoch 8/200\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 7.2932 - accuracy: 0.0082\n",
            "Epoch 9/200\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 7.2377 - accuracy: 0.0091\n",
            "Epoch 10/200\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 7.1898 - accuracy: 0.0092\n",
            "Epoch 11/200\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 7.1537 - accuracy: 0.0092\n",
            "Epoch 12/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 7.1150 - accuracy: 0.0097\n",
            "Epoch 13/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 7.0786 - accuracy: 0.0111\n",
            "Epoch 14/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 7.0389 - accuracy: 0.0124\n",
            "Epoch 15/200\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 7.0014 - accuracy: 0.0126\n",
            "Epoch 16/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 6.9616 - accuracy: 0.0132\n",
            "Epoch 17/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 6.9297 - accuracy: 0.0148\n",
            "Epoch 18/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 6.9187 - accuracy: 0.0162\n",
            "Epoch 19/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 6.9041 - accuracy: 0.0156\n",
            "Epoch 20/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 6.8890 - accuracy: 0.0187\n",
            "Epoch 21/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 6.8718 - accuracy: 0.0187\n",
            "Epoch 22/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 6.8284 - accuracy: 0.0207\n",
            "Epoch 23/200\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 6.8014 - accuracy: 0.0222\n",
            "Epoch 24/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 6.7762 - accuracy: 0.0196\n",
            "Epoch 25/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 6.7625 - accuracy: 0.0213\n",
            "Epoch 26/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 6.7457 - accuracy: 0.0244\n",
            "Epoch 27/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 6.7412 - accuracy: 0.0237\n",
            "Epoch 28/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 6.7399 - accuracy: 0.0228\n",
            "Epoch 29/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 6.7129 - accuracy: 0.0237\n",
            "Epoch 30/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 6.6471 - accuracy: 0.0276\n",
            "Epoch 31/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 6.5935 - accuracy: 0.0269\n",
            "Epoch 32/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 6.5646 - accuracy: 0.0258\n",
            "Epoch 33/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 6.5322 - accuracy: 0.0291\n",
            "Epoch 34/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 6.4883 - accuracy: 0.0321\n",
            "Epoch 35/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 6.4663 - accuracy: 0.0320\n",
            "Epoch 36/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 6.4728 - accuracy: 0.0327\n",
            "Epoch 37/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 6.4730 - accuracy: 0.0336\n",
            "Epoch 38/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 6.4553 - accuracy: 0.0344\n",
            "Epoch 39/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 6.4541 - accuracy: 0.0354\n",
            "Epoch 40/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 6.4393 - accuracy: 0.0350\n",
            "Epoch 41/200\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 6.3795 - accuracy: 0.0347\n",
            "Epoch 42/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 6.3196 - accuracy: 0.0343\n",
            "Epoch 43/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 6.2064 - accuracy: 0.0360\n",
            "Epoch 44/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 6.1274 - accuracy: 0.0397\n",
            "Epoch 45/200\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 6.0730 - accuracy: 0.0433\n",
            "Epoch 46/200\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 6.0212 - accuracy: 0.0437\n",
            "Epoch 47/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 5.9664 - accuracy: 0.0471\n",
            "Epoch 48/200\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 5.9295 - accuracy: 0.0487\n",
            "Epoch 49/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 5.9062 - accuracy: 0.0497\n",
            "Epoch 50/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 5.8739 - accuracy: 0.0496\n",
            "Epoch 51/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 5.8388 - accuracy: 0.0523\n",
            "Epoch 52/200\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 5.8133 - accuracy: 0.0515\n",
            "Epoch 53/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 5.8244 - accuracy: 0.0513\n",
            "Epoch 54/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 5.8307 - accuracy: 0.0505\n",
            "Epoch 55/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 5.8417 - accuracy: 0.0486\n",
            "Epoch 56/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 5.8140 - accuracy: 0.0513\n",
            "Epoch 57/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 5.7520 - accuracy: 0.0541\n",
            "Epoch 58/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 5.6810 - accuracy: 0.0544\n",
            "Epoch 59/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 5.5975 - accuracy: 0.0563\n",
            "Epoch 60/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 5.5318 - accuracy: 0.0613\n",
            "Epoch 61/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 5.4802 - accuracy: 0.0605\n",
            "Epoch 62/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 5.4450 - accuracy: 0.0635\n",
            "Epoch 63/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 5.4107 - accuracy: 0.0673\n",
            "Epoch 64/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 5.3887 - accuracy: 0.0661\n",
            "Epoch 65/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 5.3667 - accuracy: 0.0662\n",
            "Epoch 66/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 5.3586 - accuracy: 0.0675\n",
            "Epoch 67/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 5.3451 - accuracy: 0.0717\n",
            "Epoch 68/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 5.3320 - accuracy: 0.0686\n",
            "Epoch 69/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 5.3092 - accuracy: 0.0713\n",
            "Epoch 70/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 5.2602 - accuracy: 0.0760\n",
            "Epoch 71/200\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 5.2198 - accuracy: 0.0753\n",
            "Epoch 72/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 5.1804 - accuracy: 0.0744\n",
            "Epoch 73/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 5.1354 - accuracy: 0.0799\n",
            "Epoch 74/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 5.1051 - accuracy: 0.0804\n",
            "Epoch 75/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 5.0707 - accuracy: 0.0819\n",
            "Epoch 76/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 5.0321 - accuracy: 0.0856\n",
            "Epoch 77/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 4.9900 - accuracy: 0.0872\n",
            "Epoch 78/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 4.9463 - accuracy: 0.0926\n",
            "Epoch 79/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 4.9010 - accuracy: 0.0942\n",
            "Epoch 80/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 4.8686 - accuracy: 0.0964\n",
            "Epoch 81/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 4.8248 - accuracy: 0.1036\n",
            "Epoch 82/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 4.7852 - accuracy: 0.1050\n",
            "Epoch 83/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 4.7587 - accuracy: 0.1055\n",
            "Epoch 84/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 4.7281 - accuracy: 0.1075\n",
            "Epoch 85/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 4.7076 - accuracy: 0.1118\n",
            "Epoch 86/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 4.6738 - accuracy: 0.1124\n",
            "Epoch 87/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 4.6453 - accuracy: 0.1138\n",
            "Epoch 88/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 4.6187 - accuracy: 0.1175\n",
            "Epoch 89/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 4.6052 - accuracy: 0.1186\n",
            "Epoch 90/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 4.5708 - accuracy: 0.1236\n",
            "Epoch 91/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 4.5407 - accuracy: 0.1270\n",
            "Epoch 92/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 4.5267 - accuracy: 0.1262\n",
            "Epoch 93/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 4.4983 - accuracy: 0.1303\n",
            "Epoch 94/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 4.4683 - accuracy: 0.1310\n",
            "Epoch 95/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 4.4421 - accuracy: 0.1326\n",
            "Epoch 96/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 4.4222 - accuracy: 0.1376\n",
            "Epoch 97/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 4.3726 - accuracy: 0.1466\n",
            "Epoch 98/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 4.3324 - accuracy: 0.1457\n",
            "Epoch 99/200\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 4.2926 - accuracy: 0.1470\n",
            "Epoch 100/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 4.2510 - accuracy: 0.1535\n",
            "Epoch 101/200\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 4.2167 - accuracy: 0.1582\n",
            "Epoch 102/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 4.1777 - accuracy: 0.1627\n",
            "Epoch 103/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 4.1324 - accuracy: 0.1719\n",
            "Epoch 104/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 4.1011 - accuracy: 0.1750\n",
            "Epoch 105/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 4.0635 - accuracy: 0.1765\n",
            "Epoch 106/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 4.0215 - accuracy: 0.1841\n",
            "Epoch 107/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.9922 - accuracy: 0.1883\n",
            "Epoch 108/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.9679 - accuracy: 0.1867\n",
            "Epoch 109/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.9339 - accuracy: 0.1960\n",
            "Epoch 110/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 3.9029 - accuracy: 0.1993\n",
            "Epoch 111/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.8814 - accuracy: 0.2036\n",
            "Epoch 112/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.8907 - accuracy: 0.2038\n",
            "Epoch 113/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 3.8747 - accuracy: 0.2053\n",
            "Epoch 114/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.8425 - accuracy: 0.2181\n",
            "Epoch 115/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.8077 - accuracy: 0.2218\n",
            "Epoch 116/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.7784 - accuracy: 0.2250\n",
            "Epoch 117/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.7353 - accuracy: 0.2358\n",
            "Epoch 118/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.7157 - accuracy: 0.2339\n",
            "Epoch 119/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.6750 - accuracy: 0.2428\n",
            "Epoch 120/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.6413 - accuracy: 0.2480\n",
            "Epoch 121/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.6267 - accuracy: 0.2527\n",
            "Epoch 122/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 3.6108 - accuracy: 0.2584\n",
            "Epoch 123/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 3.5812 - accuracy: 0.2620\n",
            "Epoch 124/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.5631 - accuracy: 0.2664\n",
            "Epoch 125/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.5387 - accuracy: 0.2722\n",
            "Epoch 126/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.5314 - accuracy: 0.2707\n",
            "Epoch 127/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.5209 - accuracy: 0.2690\n",
            "Epoch 128/200\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 3.4786 - accuracy: 0.2805\n",
            "Epoch 129/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 3.4430 - accuracy: 0.2873\n",
            "Epoch 130/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 3.3945 - accuracy: 0.2974\n",
            "Epoch 131/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.3617 - accuracy: 0.3082\n",
            "Epoch 132/200\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 3.3310 - accuracy: 0.3115\n",
            "Epoch 133/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 3.3076 - accuracy: 0.3200\n",
            "Epoch 134/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 3.2737 - accuracy: 0.3255\n",
            "Epoch 135/200\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 3.2399 - accuracy: 0.3318\n",
            "Epoch 136/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.2141 - accuracy: 0.3446\n",
            "Epoch 137/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 3.1846 - accuracy: 0.3475\n",
            "Epoch 138/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 3.1508 - accuracy: 0.3613\n",
            "Epoch 139/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 3.1311 - accuracy: 0.3615\n",
            "Epoch 140/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 3.1055 - accuracy: 0.3672\n",
            "Epoch 141/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.0806 - accuracy: 0.3765\n",
            "Epoch 142/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 3.0669 - accuracy: 0.3770\n",
            "Epoch 143/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 3.0525 - accuracy: 0.3841\n",
            "Epoch 144/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 3.0195 - accuracy: 0.3873\n",
            "Epoch 145/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.9968 - accuracy: 0.3993\n",
            "Epoch 146/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.9738 - accuracy: 0.4070\n",
            "Epoch 147/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.9724 - accuracy: 0.4058\n",
            "Epoch 148/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.9361 - accuracy: 0.4135\n",
            "Epoch 149/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.9171 - accuracy: 0.4136\n",
            "Epoch 150/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.8897 - accuracy: 0.4224\n",
            "Epoch 151/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.8423 - accuracy: 0.4355\n",
            "Epoch 152/200\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 2.8249 - accuracy: 0.4440\n",
            "Epoch 153/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.7960 - accuracy: 0.4466\n",
            "Epoch 154/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.7822 - accuracy: 0.4538\n",
            "Epoch 155/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.7645 - accuracy: 0.4615\n",
            "Epoch 156/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.7455 - accuracy: 0.4621\n",
            "Epoch 157/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.7329 - accuracy: 0.4669\n",
            "Epoch 158/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.7105 - accuracy: 0.4696\n",
            "Epoch 159/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.7171 - accuracy: 0.4665\n",
            "Epoch 160/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.7116 - accuracy: 0.4635\n",
            "Epoch 161/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.6909 - accuracy: 0.4679\n",
            "Epoch 162/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.6600 - accuracy: 0.4827\n",
            "Epoch 163/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.6596 - accuracy: 0.4779\n",
            "Epoch 164/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.6086 - accuracy: 0.4939\n",
            "Epoch 165/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.5727 - accuracy: 0.4990\n",
            "Epoch 166/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.5363 - accuracy: 0.5141\n",
            "Epoch 167/200\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 2.5056 - accuracy: 0.5156\n",
            "Epoch 168/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.4804 - accuracy: 0.5286\n",
            "Epoch 169/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.4653 - accuracy: 0.5353\n",
            "Epoch 170/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.4437 - accuracy: 0.5387\n",
            "Epoch 171/200\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 2.4306 - accuracy: 0.5413\n",
            "Epoch 172/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.4157 - accuracy: 0.5448\n",
            "Epoch 173/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.4061 - accuracy: 0.5492\n",
            "Epoch 174/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.3845 - accuracy: 0.5524\n",
            "Epoch 175/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.3839 - accuracy: 0.5566\n",
            "Epoch 176/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.3609 - accuracy: 0.5588\n",
            "Epoch 177/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.3544 - accuracy: 0.5623\n",
            "Epoch 178/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.3378 - accuracy: 0.5643\n",
            "Epoch 179/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.3195 - accuracy: 0.5729\n",
            "Epoch 180/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.2960 - accuracy: 0.5813\n",
            "Epoch 181/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.2661 - accuracy: 0.5829\n",
            "Epoch 182/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.3165 - accuracy: 0.5701\n",
            "Epoch 183/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.2875 - accuracy: 0.5765\n",
            "Epoch 184/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.2474 - accuracy: 0.5875\n",
            "Epoch 185/200\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 2.2128 - accuracy: 0.5954\n",
            "Epoch 186/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.1890 - accuracy: 0.5988\n",
            "Epoch 187/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.1748 - accuracy: 0.6101\n",
            "Epoch 188/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.1723 - accuracy: 0.6060\n",
            "Epoch 189/200\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 2.1566 - accuracy: 0.6102\n",
            "Epoch 190/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.1447 - accuracy: 0.6170\n",
            "Epoch 191/200\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 2.1302 - accuracy: 0.6093\n",
            "Epoch 192/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.1009 - accuracy: 0.6204\n",
            "Epoch 193/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.0770 - accuracy: 0.6293\n",
            "Epoch 194/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.0610 - accuracy: 0.6302\n",
            "Epoch 195/200\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 2.0543 - accuracy: 0.6377\n",
            "Epoch 196/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.0446 - accuracy: 0.6340\n",
            "Epoch 197/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.0354 - accuracy: 0.6389\n",
            "Epoch 198/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.0122 - accuracy: 0.6469\n",
            "Epoch 199/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 2.0053 - accuracy: 0.6470\n",
            "Epoch 200/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 1.9912 - accuracy: 0.6513\n",
            "CPU times: user 5min 43s, sys: 52.9 s, total: 6min 36s\n",
            "Wall time: 7min 33s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1fXTEO3GJ282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "b66076ad-6391-4587-b80e-b7461068ad9e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.title('Training accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6Vc6PHgxa6Hm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0578fef0-5ec4-42cc-ae1f-3a43e4a2c0d0"
      },
      "source": [
        "seed_text = \"그래서 나는 어른들이 알아볼 수 있도록 보아 구렁이의 속을 그렸다\"\n",
        "next_words = 100\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "    output_word = tokenizer.index_word[predicted[0]]\n",
        "    seed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "그래서 나는 어른들이 알아볼 수 있도록 보아 구렁이의 속을 그렸다 본 적이 있겠지 여우는말했다 세상에 걸 있어 그 꽃은 나에게 이렇게 말했다 사람이거든 그는 다시 생각했다 알지 못하는 했다 했다 않고 어린 왕자가 물었다 가책을 하늘로 향했다 충분할 것이었다 그것은 이 사람이 그려 내 꽃은 기록하지 않아 진실된 잦아들듯 잦아들듯 미끄러져 가벼운 금속성 소리를 내며 돌들 사이로 는데 밖으로 시작할거 개란 장군의 잘못이 아니니라 하게 되겠지 없는 조금 들썩하면서 어린 왕자가 말했다 이렇게말하기도 했다 증거야 라고 말한다면 그들은 어깨를 아 다른 날들과 다르게 만들고 있어 그 훨씬 더 말아 견뎌야지 충분할 거야 그래서 네가 자신을 보호하는 거야 그래서 아저씨의 없으니까 정말로 뜻이지 열 둘 있네요 것이었다 그것은 별이 여기 하지\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OX_yyEP95aKZ",
        "colab": {}
      },
      "source": [
        "\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}